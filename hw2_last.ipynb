{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "068e3831",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36483d35",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#wget.download('http://deepyeti.ucsd.edu/jianmo/amazon/categoryFiles/All_Beauty.json.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "24eedacd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5269\n",
      "As advertised. Reasonably priced\n",
      "5264\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import gzip\n",
    "import json\n",
    "\n",
    "data = []\n",
    "with gzip.open('All_Beauty_5.json.gz') as f:\n",
    "    for l in f:\n",
    "        data.append(json.loads(l.strip()).get('reviewText'))\n",
    "\n",
    "print(len(data))\n",
    "\n",
    "print(data[0])\n",
    "\n",
    "data = [x for x in data if x!=None]\n",
    "\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d2f559d",
   "metadata": {},
   "source": [
    "В чём, собственно, проблема, о наличии которой можно подумать, увидев это высказывание -- в отзывы на конкретный продукт сам продукт можно и не упоминать"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13b09feb",
   "metadata": {},
   "source": [
    "# 1. Как найти товары: способы"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7df9d24",
   "metadata": {},
   "source": [
    "*(3 балла) Предложите 3 способа найти упоминания товаров в отзывах. Например, использовать bootstrapping: составить шаблоны вида \"холодильник XXX\", найти все соответствующие n-граммы и выделить из них называние товара. Могут помочь заголовки и дополнительные данные с Amazon (Metadata здесь) Какие данные необходимы для каждого из способов? Какие есть достоинства/недостатки?*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e6a3c01",
   "metadata": {},
   "source": [
    "__1. Читать и размечать, как, например, именованные сущности. После определённого количества размеченного можно попробовать обучить модель.__\n",
    " <br>\n",
    "Плюсы: точно; не надо быть лингвистом; можно отдать на Толоку<br>\n",
    "Минусы: долго и трудозатртно; можеть быть дорого; может получиться далеко не так точно, как предполагалось, из-за несогласия разметчиков или неясности правил<br>\n",
    " <br>\n",
    "__2. Сложить всё в ворд-ту-век, надеяться на то, что упоминания товаров окажутся там в одном кластере__<br>\n",
    " <br>\n",
    "Плюсы: Если повезёт, можно сгруппировать упоминания одного товара/синонимы. Относительно быстро. Не надо привлекать людей<br>\n",
    "Минусы: Упоминания товаров часто не однословны, нужно что-то с биграммами придумывать. Или с триграммами. <br>\n",
    " <br>\n",
    "__3. Привлечь тяжёлую артиллерию, а именно млодели для извлечения именованных сущностей__<br>\n",
    " <br>\n",
    "Плюсы: Есть вероятность, что получится неплохая точность. Можно расширять список за счёт применения новых и новых идей<br>\n",
    "Минусы: сложно, получится, скорее всего, не идеально\n",
    " <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5338715",
   "metadata": {},
   "source": [
    "# Извлечение названий товаров"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f336e13f",
   "metadata": {},
   "source": [
    "*(2 балла) Реализуйте один из предложенных вами способов.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c711424e",
   "metadata": {},
   "source": [
    "Ну что ж, попробуем. Посмотрим на наши данные!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0edd611e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[30:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "508ec562",
   "metadata": {},
   "source": [
    "Здесь выделяются три типа упоминаний. <br>\n",
    "1. Никак не упоминаем продукт или называем его \"it\"\n",
    "2. this NOUN, ADJ + NOUN\n",
    "3. Собственно названия\n",
    " <br>\n",
    " \n",
    "И что же мы будем с этим делать? Ну, давайте действовать в два этапа. Даже в три. Пункт первый -- постараться извлечь именованные сущности и посмотреть на них"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b577275f",
   "metadata": {},
   "source": [
    "### Достаём именованные сущности"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "970145c2",
   "metadata": {},
   "source": [
    "Stanza очень хороша! Но при этом она работает достаточно долго. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "a5453283",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-07 04:38:56 INFO: Loading these models for language: en (English):\n",
      "=========================\n",
      "| Processor | Package   |\n",
      "-------------------------\n",
      "| tokenize  | combined  |\n",
      "| ner       | ontonotes |\n",
      "=========================\n",
      "\n",
      "2021-12-07 04:38:56 INFO: Use device: gpu\n",
      "2021-12-07 04:38:56 INFO: Loading: tokenize\n",
      "2021-12-07 04:38:56 INFO: Loading: ner\n",
      "2021-12-07 04:38:56 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "import stanza\n",
    "from tqdm import tqdm\n",
    "#stanza.download('en')\n",
    "nlp = stanza.Pipeline(lang='en', processors='tokenize,ner')\n",
    "def stanza_result(text, nlp=nlp):\n",
    "    doc = nlp(text)\n",
    "    ent_texts = [ent.text for sent in doc.sentences for ent in sent.ents]\n",
    "    ent_types = [ent.type for sent in doc.sentences for ent in sent.ents]\n",
    "    #print(*[f'entity: {ent.text}\\ttype: {ent.type}' for sent in doc.sentences for ent in sent.ents], sep='\\n')\n",
    "    return ent_texts, ent_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "9f46473a",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉              | 4693/5264 [06:35<00:48, 11.87it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 72.00 MiB (GPU 0; 2.00 GiB total capacity; 680.95 MiB already allocated; 0 bytes free; 728.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_14640/3808912527.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mel\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;31m#print(i)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0ma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstanza_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[0ment_texts\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0ment_types\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_14640/587503532.py\u001b[0m in \u001b[0;36mstanza_result\u001b[1;34m(text, nlp)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mnlp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstanza\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPipeline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlang\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'en'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprocessors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'tokenize,ner'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mstanza_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnlp\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnlp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m     \u001b[0ment_texts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0ment\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdoc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msentences\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ment\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0ments\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0ment_types\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0ment\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtype\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdoc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msentences\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ment\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0ments\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\pt1\\lib\\site-packages\\stanza\\pipeline\\core.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, doc)\u001b[0m\n\u001b[0;32m    229\u001b[0m         assert any([isinstance(doc, str), isinstance(doc, list),\n\u001b[0;32m    230\u001b[0m                     isinstance(doc, Document)]), 'input should be either str, list or Document'\n\u001b[1;32m--> 231\u001b[1;33m         \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprocess\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    232\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mdoc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    233\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\pt1\\lib\\site-packages\\stanza\\pipeline\\core.py\u001b[0m in \u001b[0;36mprocess\u001b[1;34m(self, doc)\u001b[0m\n\u001b[0;32m    223\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprocessors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocessor_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    224\u001b[0m                 \u001b[0mprocess\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprocessors\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mprocessor_name\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbulk_process\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mbulk\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprocessors\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mprocessor_name\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprocess\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 225\u001b[1;33m                 \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprocess\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    226\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mdoc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    227\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\pt1\\lib\\site-packages\\stanza\\pipeline\\ner_processor.py\u001b[0m in \u001b[0;36mprocess\u001b[1;34m(self, document)\u001b[0m\n\u001b[0;32m     33\u001b[0m         \u001b[0mpreds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m             \u001b[0mpreds\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     36\u001b[0m         \u001b[0mbatch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mNER\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0my\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpreds\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0my\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mto_token\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m         \u001b[1;31m# collect entities into document attribute\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\pt1\\lib\\site-packages\\stanza\\models\\ner\\trainer.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, batch, unsort)\u001b[0m\n\u001b[0;32m     96\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m         \u001b[0mbatch_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 98\u001b[1;33m         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogits\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrans\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword_mask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwordchars\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwordchars_mask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtags\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword_orig_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msentlens\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwordlens\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchars\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcharoffsets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcharlens\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchar_orig_idx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     99\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m         \u001b[1;31m# decode\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\pt1\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\pt1\\lib\\site-packages\\stanza\\models\\ner\\model.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, word, word_mask, wordchars, wordchars_mask, tags, word_orig_idx, sentlens, wordlens, chars, charoffsets, charlens, char_orig_idx)\u001b[0m\n\u001b[0;32m     99\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'char'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'char_emb_dim'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'charlm'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 101\u001b[1;33m                 \u001b[0mchar_reps_forward\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcharmodel_forward\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_representation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchars\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcharoffsets\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcharlens\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchar_orig_idx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    102\u001b[0m                 \u001b[0mchar_reps_forward\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPackedSequence\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchar_reps_forward\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchar_reps_forward\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_sizes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    103\u001b[0m                 \u001b[0mchar_reps_backward\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcharmodel_backward\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_representation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchars\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcharoffsets\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcharlens\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchar_orig_idx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\pt1\\lib\\site-packages\\stanza\\models\\common\\char_model.py\u001b[0m in \u001b[0;36mget_representation\u001b[1;34m(self, chars, charoffsets, charlens, char_orig_idx)\u001b[0m\n\u001b[0;32m     96\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_representation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchars\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcharoffsets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcharlens\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchar_orig_idx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 98\u001b[1;33m             \u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchars\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcharlens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     99\u001b[0m             \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moffsets\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moffsets\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcharoffsets\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m             \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0munsort\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mres\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchar_orig_idx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\pt1\\lib\\site-packages\\stanza\\models\\common\\char_model.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, chars, charlens, hidden)\u001b[0m\n\u001b[0;32m     91\u001b[0m         \u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcharlstm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0membs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcharlens\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mhidden\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpad_packed_sequence\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_first\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 93\u001b[1;33m         \u001b[0mdecoded\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     94\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecoded\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\pt1\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\pt1\\lib\\site-packages\\torch\\nn\\modules\\linear.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    102\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 103\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    104\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    105\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\pt1\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mlinear\u001b[1;34m(input, weight, bias)\u001b[0m\n\u001b[0;32m   1846\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhas_torch_function_variadic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1847\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1848\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1849\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1850\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 72.00 MiB (GPU 0; 2.00 GiB total capacity; 680.95 MiB already allocated; 0 bytes free; 728.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "ent_texts = [] \n",
    "ent_types = []\n",
    "for el in tqdm(data):\n",
    "    #print(i)\n",
    "    a = stanza_result(el)\n",
    "    ent_texts.extend(a[0])\n",
    "    ent_types.extend(a[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ea0afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "products = []\n",
    "for i, el in enumerate(ent_types):\n",
    "    if el == 'PRODUCT':\n",
    "        products.append(i)\n",
    "        print(ent_texts[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ccecb34",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(products)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "189182da",
   "metadata": {},
   "source": [
    "### Часть вторая"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ca4ced",
   "metadata": {},
   "source": [
    "Во-первых, stanza достатчно хороша в доставании названий продуктов. Даже очень хороша. Временами она достаёт названия фирм или компаний -- но во многих случаях это довольно логично: многие упоминания выглядят, как \"шампунь X\". А нам такое не очень нравится, наверное, потому что мы, вероятно, хотим знать, какой именно это продукт. Поэтому давайте экспериментировать с морфологией и отзывами.<br>\n",
    " <br>\n",
    "Можно сразу начать думать обо всех пунктах задания. Что нам, в общем-то, нужно? Нам нужны упоминания товаров. Желательно всех товаров, которые упоминаются в отзыве, а он там может быть не один. Чтобы такое добыть и, например, сформировать хорошие N-граммы с сущностями, по которым потом тоже можно искать, можно посмотреть на то, как наши упоминания распределяются по отзывам."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3df2aef",
   "metadata": {},
   "source": [
    "(1 балл) Соберите n-граммы с полученными сущностями (NE + левый сосед / NE + правый сосед)<br>\n",
    " <br>\n",
    "(3 балла) Ранжируйте n-граммы с помощью 3 коллокационных метрик (t-score, PMI и т.д.). Не забудьте про частотный фильтр / сглаживание. Выберите лучший результат (какая метрика ранжирует выше коллокации, подходящие для отчёта).<br>\n",
    " <br>\n",
    "(1 балл) Сгруппируйте полученные коллокации по NE, выведите примеры для 5 товаров. Должны получиться примерно такие группы:<br>\n",
    " <br>\n",
    "(2 балла): если придумаете способ объединить синонимичные упоминания (например, \"Samsung Galaxy Watch\", \"watch\", \"smartwatch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f1605b3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-07 00:12:26 INFO: Loading these models for language: en (English):\n",
      "=========================\n",
      "| Processor | Package   |\n",
      "-------------------------\n",
      "| tokenize  | combined  |\n",
      "| pos       | combined  |\n",
      "| lemma     | combined  |\n",
      "| ner       | ontonotes |\n",
      "=========================\n",
      "\n",
      "2021-12-07 00:12:26 INFO: Use device: gpu\n",
      "2021-12-07 00:12:26 INFO: Loading: tokenize\n",
      "2021-12-07 00:12:28 INFO: Loading: pos\n",
      "2021-12-07 00:12:28 INFO: Loading: lemma\n",
      "2021-12-07 00:12:28 INFO: Loading: ner\n",
      "2021-12-07 00:12:28 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "import stanza\n",
    "from tqdm import tqdm\n",
    "#stanza.download('en')\n",
    "nlp = stanza.Pipeline(lang='en', processors='tokenize,lemma,pos,ner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e878c879",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stanza_doc(text, nlp=nlp):\n",
    "    doc = nlp(text)\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92646372",
   "metadata": {},
   "source": [
    "Берём ещё организации, чтобы было полнее -- stanza распознаёт часть упоминаний вроде \"продукт Шанель\" как \"организация Шанель\". В препроцессинге происходит удаление всех предлогов, собзов, артиклей -- всё то, что в школе бы назвали служебными частями речи. Здесь одновременно происходит добывание упоминаний и собирание n-грамм"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "68212619",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stanza_result(doc):\n",
    "    ent_texts = [ent.text for sent in doc.sentences for ent in sent.ents]\n",
    "    ent_types = [ent.type for sent in doc.sentences for ent in sent.ents]\n",
    "    ent_ngrams = []\n",
    "    this_list_1 = []\n",
    "    this_list_2 = []\n",
    "    one_ngram = []\n",
    "    stops = ['ADP', 'AUX', 'CCONJ', 'DET', 'INTJ', 'PART', 'PRON', 'PUNCT', 'SCONJ']\n",
    "    for sent in doc.sentences:\n",
    "        #list_tokens = list(token for token in sent.tokens if token.upos != 'PUNCT')\n",
    "        list_tokens = list(token for token in sent.tokens if token.words[0].upos not in stops)\n",
    "        for i, token in enumerate(list_tokens):\n",
    "            if token.words[0].lemma == 'this':\n",
    "                if i + 1 < len(list_tokens):\n",
    "                    this_list_1.append(list_tokens[i+1])\n",
    "                    if i + 2 < len(list_tokens):\n",
    "                        this_list_2.append(list_tokens[i+2])\n",
    "        #print(sent.ents)\n",
    "            if token.ner == 'B-ORG' or token.ner == 'B-PRODUCT':\n",
    "                ent_ngrams.append(one_ngram)\n",
    "                one_ngram = []\n",
    "                if i > 0:\n",
    "                    one_ngram.append(list_tokens[i-1])\n",
    "                else:\n",
    "                    one_ngram.append('START')\n",
    "                one_ngram.append(token)\n",
    "            elif token.ner == 'I-ORG' or token.ner == 'I-PRODUCT':\n",
    "                one_ngram.append(token)\n",
    "            elif token.ner == 'E-ORG' or token.ner == 'E-PRODUCT':\n",
    "                one_ngram.append(token)\n",
    "                if i + 1 < len(list_tokens):\n",
    "                    one_ngram.append(list_tokens[i+1])\n",
    "                else:\n",
    "                    one_ngram.append('END')\n",
    "            elif token.ner == 'S-ORG' or token.ner == 'S-PRODUCT':\n",
    "                ent_ngrams.append(one_ngram)\n",
    "                one_ngram = []\n",
    "                if i > 0:\n",
    "                    one_ngram.append(list_tokens[i-1])\n",
    "                else:\n",
    "                    one_ngram.append('START')\n",
    "                one_ngram.append(token)\n",
    "                if i + 1 < len(list_tokens):\n",
    "                    one_ngram.append(list_tokens[i+1])\n",
    "                else:\n",
    "                    one_ngram.append('END')\n",
    "        \n",
    "    ent_ngrams.append(one_ngram)\n",
    "\n",
    "            \n",
    "    \n",
    "    \n",
    "    #print(*[f'entity: {ent.text}\\ttype: {ent.type}' for sent in doc.sentences for ent in sent.ents], sep='\\n')\n",
    "    return ent_texts, ent_types, ent_ngrams, this_list_1, this_list_2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4e4bd338",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5264/5264 [11:42<00:00,  7.49it/s]\n"
     ]
    }
   ],
   "source": [
    "stanza_docs = []\n",
    "for text in tqdm(data):\n",
    "    stanza_docs.append(stanza_doc(text, nlp=nlp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cc3ab8a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5264/5264 [11:38<00:00,  7.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 11min 38s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "all_ent_texts = []\n",
    "all_ent_types = []\n",
    "all_ent_ngrams = []\n",
    "all_this_list_1 = []\n",
    "all_this_list_2 = []\n",
    "\n",
    "for text in tqdm(data):\n",
    "    ent_texts, ent_types, ent_ngrams, this_list_1, this_list_2 = stanza_result(stanza_doc(text, nlp=nlp))\n",
    "    all_ent_texts.append(ent_texts)\n",
    "    all_ent_types.append(ent_types)\n",
    "    all_ent_ngrams.append(ent_ngrams)\n",
    "    all_this_list_1.append(this_list_1)\n",
    "    all_this_list_2.append(this_list_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6a7d33b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_text = []\n",
    "stops = ['ADP', 'AUX', 'CCONJ', 'DET', 'INTJ', 'PART', 'PRON', 'PUNCT', 'SCONJ']\n",
    "for one_doc in stanza_docs:\n",
    "    for sent in one_doc.sentences:\n",
    "        all_text.extend([word.text.lower() for word in sent.words if word.upos not in stops])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15347163",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e05c940c",
   "metadata": {},
   "source": [
    "## Считаем метрики"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "cc118a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_freq(our_ngram, all_texts):\n",
    "    stops = ['START', 'END']\n",
    "    left_context = len([i for i in all_text if i == our_ngram[0] and not i in stops])\n",
    "    right_context = len([i for i in  all_text if i == our_ngram[-1] and not i in stops])\n",
    "    our_ent_freq = 0\n",
    "    our_ent = our_ngram[1:-1]\n",
    "    \n",
    "    index_first = []\n",
    "    index_second = []\n",
    "    if len(our_ent) == 1:\n",
    "        our_ent = our_ent[0]\n",
    "        for word in all_text:\n",
    "            if our_ent == word:\n",
    "                our_ent_freq += 1\n",
    "    \n",
    "    else:\n",
    "        for i, word in enumerate(all_texts):\n",
    "            if word == our_ent[0]:\n",
    "                index_first.append(i)\n",
    "                \n",
    "        for i, word in enumerate(our_ent[1:]):\n",
    "            for el in index_first:\n",
    "                if all_texts[el + i + 1] == word:\n",
    "                    index_second.append(el)\n",
    "            index_first = index_second\n",
    "            index_second = []\n",
    "        our_ent_freq = len(index_first)\n",
    "        \n",
    "    return left_context, right_context, our_ent_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "7572a770",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_freq_ngram(our_ngram, all_texts):\n",
    "    index_first = []\n",
    "    index_second = []    \n",
    "    for i, word in enumerate(all_texts):\n",
    "        if word == our_ngram[0]:\n",
    "            index_first.append(i)\n",
    "\n",
    "    for i, word in enumerate(our_ngram[1:]):\n",
    "        for el in index_first:\n",
    "            if all_texts[el + i + 1] == word:\n",
    "                index_second.append(el)\n",
    "        index_first = index_second\n",
    "        index_second = []\n",
    "        \n",
    "    return len(index_first)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "12050b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_word_ngrams = []\n",
    "\n",
    "for one_text in all_ent_ngrams:\n",
    "    for one_ngram in one_text:\n",
    "        one_ngram_words = []\n",
    "        for word in one_ngram:\n",
    "            if word == 'START' or word == 'END':\n",
    "                 one_ngram_words.append(word)\n",
    "            else:\n",
    "                one_ngram_words.append(word.text)\n",
    "        if one_ngram_words != []:\n",
    "            all_word_ngrams.append(one_ngram_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "72633b91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Williams', 'Company', 'ran'],\n",
       " ['is', 'AQUA', 'VELVA', 'MAN', 'seemed'],\n",
       " ['negative', 'Aqua', 'Velva', 'last'],\n",
       " ['have', 'Ambergris', 'quite'],\n",
       " ['one', 'Wal', 'Mart', 'glass'],\n",
       " ['START', 'Selenium', 'appeared'],\n",
       " ['get', 'Amazon', 'END'],\n",
       " ['have', 'Airbnb', 'END'],\n",
       " ['Dave', 'Buster'],\n",
       " ['START', 'Uber', 'pleased', 'D&G', 'Lifht', 'Blue', 'one'],\n",
       " ['pleased', 'Uhuru', 'Naturals', 'Hair'],\n",
       " ['START', 'Google', 'uses'],\n",
       " ['well', 'Northern', 'Fir'],\n",
       " ['START', 'Eau', 'de', 'Hadrien', 'favorite'],\n",
       " ['Tangle', 'Taming', 'Conditioner', 'good'],\n",
       " ['tried', 'Kerastase', 'PHYTO'],\n",
       " ['Kerastase', 'PHYTO', 'Davines'],\n",
       " ['wish', 'Redken', 'discontinued'],\n",
       " ['Federal',\n",
       "  'Trade',\n",
       "  'Commission',\n",
       "  'Guidelines',\n",
       "  'Testimony',\n",
       "  'Advertising',\n",
       "  'END'],\n",
       " ['ago', 'Sally', 'Beauty', 'Supply', 'END'],\n",
       " ['PRELL', 'Conditioner', 'far'],\n",
       " ['company', 'Amazon', 'buy'],\n",
       " ['discontinued', 'Bain', 'De', 'Terre', 'END'],\n",
       " ['used', 'Mousse', 'products'],\n",
       " ['START', 'Wish', 'Walgreens', 'kept'],\n",
       " ['use', 'Pureology', 'END'],\n",
       " ['product', 'Pantene', 'made'],\n",
       " ['START', 'Perlier', 'makes'],\n",
       " ['bought', 'Big', 'Lots', 'only'],\n",
       " ['bought', 'Channel', '5', 'thinking'],\n",
       " ['like', 'H20', 'shampoo'],\n",
       " ['down', 'Disney', 'World', 'END'],\n",
       " ['loved', 'Disney', 'world'],\n",
       " ['START', 'HALE', 'BERRY', 'Pure', 'Orchid', 'wonderful'],\n",
       " ['goodness', 'Amazon', 'END'],\n",
       " ['using', 'Calibra', 'eye'],\n",
       " ['looked', 'Amazon', ';-)'],\n",
       " ['business', 'Amazon', 'END'],\n",
       " ['found', 'Amazon', 'END'],\n",
       " ['Mother', 'Christmas', 'loves'],\n",
       " ['using', 'Neutrogena', 'remove'],\n",
       " ['kit', 'Bare', 'Escentuals', 'END'],\n",
       " ['product', 'Sally', 'Beauty', 'Supply', 'END'],\n",
       " ['have', 'Green', 'Tea', 'cologne', 'Green', 'Tea', 'Body'],\n",
       " ['same', 'Green', 'Tea', 'END'],\n",
       " ['original', 'Creme', 'Perfume', 'tube', 'Body', 'Lotion', 'END'],\n",
       " ['making', '32oz', 'END'],\n",
       " ['well', 'Bare', 'Minerals', 'loose'],\n",
       " ['original',\n",
       "  'Matrix',\n",
       "  'Amplifying',\n",
       "  '/',\n",
       "  'Volumizing',\n",
       "  'System',\n",
       "  'Shampoo',\n",
       "  'number'],\n",
       " ['better', 'Matrix', 'new'],\n",
       " ['START', 'Matrix', 'never'],\n",
       " ['shampoo', 'Detangler', 'Conditioner'],\n",
       " ['local', 'Sally', 'Beauty', 'Supply', 'grocery'],\n",
       " ['use', 'Estee', 'Lauder', 'products'],\n",
       " ['products', 'Amnazon', 'great'],\n",
       " ['use', 'Estee', 'Lauder', 'foundation'],\n",
       " ['together', 'MOP', 'c-straight'],\n",
       " ['use', 'Matrix', 'conditioner'],\n",
       " ['START', 'Matrix', 'Biolage', 'Fortifying', 'conditioner'],\n",
       " ['worn', 'Chanel', '#', '5', '12'],\n",
       " ['always', 'Bare', 'Minerals', 'never'],\n",
       " ['recommended', 'HBL', 'products'],\n",
       " ['using', 'HBL', 'volumizing'],\n",
       " ['Shine', 'Seal', 'END'],\n",
       " ['Reflection', 'Chroma', 'Thermique', 'also'],\n",
       " ['used', 'Biolage', 'products'],\n",
       " ['consider', 'Macallan', 'single'],\n",
       " ['again', 'Amazon', 'thanks'],\n",
       " ['here', 'Amazon', 'END'],\n",
       " ['Bvlgari', 'Conditioner', 'favorite'],\n",
       " ['stayed', 'Ritz-Carlton', 'Mexico'],\n",
       " ['buy', 'Kerastase', 'Fusio-', 'Dose', 'Systeme', 'Professionnel', 'END'],\n",
       " ['Sea', 'Salt', 'Kelp', 'Winter', 'Winter', 'Spice', 'best'],\n",
       " ['review', 'Sea', 'Salt', 'w', 'Kelp', 'soap'],\n",
       " ['START', 'NARS', 'treasure'],\n",
       " ['Orgasm', 'NARS', 'glowing'],\n",
       " ['look', 'Avalon', 'Active', 'Organics', 'Vitamin'],\n",
       " ['Certified', 'Organic', 'Lavender', 'Floral', 'Water', 'Herbal'],\n",
       " ['Water',\n",
       "  'Herbal',\n",
       "  'Infusion',\n",
       "  'Certified',\n",
       "  'Organic',\n",
       "  'Green',\n",
       "  'Tea',\n",
       "  'Chamomile'],\n",
       " ['Tea', 'Chamomile', 'Althea'],\n",
       " ['Chamomile', 'Althea', 'Sage'],\n",
       " ['Althea', 'Sage', 'yarrow'],\n",
       " ['yarrow', 'Caprylic', '/'],\n",
       " ['Oil', 'Shea', 'Butter', 'Certified'],\n",
       " ['Organic', 'Rosemary', 'Extract', 'Sorbitol'],\n",
       " ['Extract', 'Sorbitol', 'NaPCA'],\n",
       " ['Sorbitol', 'NaPCA', 'Tocopheryl'],\n",
       " ['look', 'Avalon', 'Active', 'Organics', 'Vitamin'],\n",
       " ['Certified', 'Organic', 'Lavender', 'Floral', 'Water', 'Herbal'],\n",
       " ['Water',\n",
       "  'Herbal',\n",
       "  'Infusion',\n",
       "  'Certified',\n",
       "  'Organic',\n",
       "  'Green',\n",
       "  'Tea',\n",
       "  'Chamomile'],\n",
       " ['Tea', 'Chamomile', 'Althea'],\n",
       " ['Chamomile', 'Althea', 'Sage'],\n",
       " ['Althea', 'Sage', 'yarrow'],\n",
       " ['yarrow', 'Caprylic', '/'],\n",
       " ['Oil', 'Shea', 'Butter', 'Certified'],\n",
       " ['Organic', 'Rosemary', 'Extract', 'Sorbitol'],\n",
       " ['Extract', 'Sorbitol', 'NaPCA'],\n",
       " ['Sorbitol', 'NaPCA', 'Tocopheryl'],\n",
       " ['makes', 'Zum', 'Bars', 'luxury'],\n",
       " ['makes', 'Zum', 'Bars', 'luxury'],\n",
       " ['makes', 'Zum', 'Bars', 'luxury'],\n",
       " ['makes', 'Zum', 'Bars', 'luxury'],\n",
       " ['body', 'Spongelle', 'END'],\n",
       " ['prolong', 'Spongelle', 'useful'],\n",
       " ['see', 'P&G', 'bring'],\n",
       " ['favorite', 'Bath', 'Body'],\n",
       " ['Bath', 'Body', 'Works', 'collection'],\n",
       " ['loss', 'Amazon', 'marketplaces'],\n",
       " ['whey', 'Old', 'Spice', 'discontinued'],\n",
       " ['Thanks', 'Amazon', 'no'],\n",
       " ['START', 'Amazon', 'fixed'],\n",
       " ['kinds', 'Dial', 'Soap', 'all'],\n",
       " ['single', 'Coloniali', 'product'],\n",
       " ['par', 'Acqua', 'di', 'Parma', 'best'],\n",
       " ['Along', 'Olay', 'Age', 'Defying', 'moisturizer'],\n",
       " ['selling', 'Azur', 'brand'],\n",
       " ['find', 'Amazon', 'store'],\n",
       " ['price', 'Amazon', 'went'],\n",
       " ['buy', 'Tree', 'Hut', 'END'],\n",
       " ['Thank', 'Amazon', 'carrying'],\n",
       " ['START', 'Bath', 'Body'],\n",
       " ['Bath', 'Body', 'Works', 'quit'],\n",
       " ['sold', 'Amazon', 'END'],\n",
       " ['START', 'Monoi', 'Tiare', 'Tahiti', 'Coconut', 'Oil'],\n",
       " ['has', 'Tiare', 'flower'],\n",
       " ['scent', 'Bath', 'Body', 'Works', 'formula'],\n",
       " ['well', 'Vitamin', 'E', 'actual'],\n",
       " ['thing', 'B&BW', 'formula'],\n",
       " ['smell', 'Dial', 'Mountain', 'Fresh', 'also'],\n",
       " ['promotes', 'Antibacterial', 'Wash', 'END'],\n",
       " ['received', 'Popsugar', 'box'],\n",
       " ['START', 'Fragarant', 'strong'],\n",
       " ['went', 'Amazon', 'ordered'],\n",
       " ['price', 'Hermes', 'product'],\n",
       " ['START', 'Henkel', 'quit'],\n",
       " ['worth', 'Amazon', 'no'],\n",
       " ['favorite', 'Thymes', 'collection'],\n",
       " ['START', 'Azur', 'discontinued'],\n",
       " ['travel', 'TSA', 'regulations'],\n",
       " ['disappointed', 'BedBathandBeyond', 'discontinued'],\n",
       " ['find', 'Amazon', 'END'],\n",
       " ['buy', 'Moon', 'Sparkle', 'again'],\n",
       " ['lotion', 'Ross', 'store'],\n",
       " ['expensive', 'Amazon', 'still'],\n",
       " ['love', 'Amazon', 'only'],\n",
       " ['Shower', 'Gel', 'layer'],\n",
       " ['is', 'Shower', 'Gel', 'fragrance'],\n",
       " ['love', 'Body', 'Shop', 'items'],\n",
       " ['get', 'Amazon', 'END'],\n",
       " ['Body', 'Shop'],\n",
       " ['START', 'Amazon', 'price'],\n",
       " ['found', 'Amazon', 'END'],\n",
       " ['sold', 'Amazon.com', 'END'],\n",
       " ['order', 'Amazon', 'cost'],\n",
       " ['salad', 'Whole', 'Foods', 'body'],\n",
       " ['best', 'Axe', 'line'],\n",
       " ['love',\n",
       "  'Clarins',\n",
       "  'Eau',\n",
       "  'Ensoleillante',\n",
       "  'Moisturizing',\n",
       "  'Body',\n",
       "  'Lotion',\n",
       "  'END'],\n",
       " ['disappointed', 'Avon', 'discontinued'],\n",
       " ['items', 'Body', 'shop'],\n",
       " ['shop', 'Bath', 'Body', 'Works', 'END'],\n",
       " ['Thanks', 'Amazon', 'sites'],\n",
       " ['favorite', 'Bath', 'Body', 'Works', 'scent'],\n",
       " ['gels', 'AXE', 'END'],\n",
       " ['acquainted', 'Savannah', 'Bee', 'Mint', 'Julep', 'originally'],\n",
       " ['one', 'Bath', 'Body', 'END'],\n",
       " ['buy', 'Amazon', 'found'],\n",
       " ['awards', 'Pattern', 'Body', 'Wash', 'received'],\n",
       " ['many', 'Boots', 'products'],\n",
       " ['bottle', \"J'Adore\", 'lotion'],\n",
       " ['START', 'Dove', 'no'],\n",
       " ['find', 'Amazon', 'END'],\n",
       " ['product', 'Bath', 'Body', 'one'],\n",
       " ['know', 'Bath', 'Body', 'stopped'],\n",
       " ['find', 'Amazon', 'has'],\n",
       " ['now', 'Amazon', 'not'],\n",
       " ['find', 'Amazon', 'END'],\n",
       " ['years', 'Thymes', 'discontinued'],\n",
       " ['using', 'Vitamin', 'E', 'gel'],\n",
       " ['take', 'Softsoap', 'grain'],\n",
       " ['fan', 'Olay', 'orchid'],\n",
       " ['see', 'Amazon', 'sells'],\n",
       " ['such', 'Sephora', 'other'],\n",
       " ['shipped', 'FPO', 'address'],\n",
       " ['single', 'Escada', 'fragrance'],\n",
       " ['cheaper', 'B&BW', 'END'],\n",
       " ['START', 'Thymes', 'comany'],\n",
       " ['production', 'Filigree', 'glad'],\n",
       " ['day', 'SPA', 'LOVE'],\n",
       " ['using', 'BeautiControl', 'time'],\n",
       " ['fan', 'Molton', 'Brown', 'products'],\n",
       " ['Reviving', 'Cempaka', 'Bath', 'Shower', 'gel'],\n",
       " ['However', 'Tea', 'Tree', 'often'],\n",
       " ['purchased', 'Chanel', 'Antaeus', 'Bath', 'shower'],\n",
       " ['shower', 'Gel', 'END'],\n",
       " ['used', 'Bellmira', 'Herbaflor', 'Herbal', 'Baths', 'years'],\n",
       " ['gel', 'Solstis', 'really'],\n",
       " ['body', 'Spongelle', 'END'],\n",
       " ['prolong', 'Spongelle', 'useful'],\n",
       " ['see', 'P&G', 'bring'],\n",
       " ['favorite', 'Bath', 'Body'],\n",
       " ['Bath', 'Body', 'Works', 'collection'],\n",
       " ['loss', 'Amazon', 'marketplaces'],\n",
       " ['whey', 'Old', 'Spice', 'discontinued'],\n",
       " ['Thanks', 'Amazon', 'no'],\n",
       " ['START', 'Amazon', 'fixed'],\n",
       " ['kinds', 'Dial', 'Soap', 'all'],\n",
       " ['single', 'Coloniali', 'product'],\n",
       " ['par', 'Acqua', 'di', 'Parma', 'best'],\n",
       " ['Along', 'Olay', 'Age', 'Defying', 'moisturizer'],\n",
       " ['selling', 'Azur', 'brand'],\n",
       " ['find', 'Amazon', 'store'],\n",
       " ['price', 'Amazon', 'went'],\n",
       " ['buy', 'Tree', 'Hut', 'END'],\n",
       " ['Thank', 'Amazon', 'carrying'],\n",
       " ['START', 'Bath', 'Body'],\n",
       " ['Bath', 'Body', 'Works', 'quit'],\n",
       " ['sold', 'Amazon', 'END'],\n",
       " ['START', 'Monoi', 'Tiare', 'Tahiti', 'Coconut', 'Oil'],\n",
       " ['has', 'Tiare', 'flower'],\n",
       " ['scent', 'Bath', 'Body', 'Works', 'formula'],\n",
       " ['well', 'Vitamin', 'E', 'actual'],\n",
       " ['thing', 'B&BW', 'formula'],\n",
       " ['smell', 'Dial', 'Mountain', 'Fresh', 'also'],\n",
       " ['promotes', 'Antibacterial', 'Wash', 'END'],\n",
       " ['received', 'Popsugar', 'box'],\n",
       " ['START', 'Fragarant', 'strong'],\n",
       " ['went', 'Amazon', 'ordered'],\n",
       " ['price', 'Hermes', 'product'],\n",
       " ['START', 'Henkel', 'quit'],\n",
       " ['worth', 'Amazon', 'no'],\n",
       " ['favorite', 'Thymes', 'collection'],\n",
       " ['START', 'Azur', 'discontinued'],\n",
       " ['travel', 'TSA', 'regulations'],\n",
       " ['disappointed', 'BedBathandBeyond', 'discontinued'],\n",
       " ['find', 'Amazon', 'END'],\n",
       " ['buy', 'Moon', 'Sparkle', 'again'],\n",
       " ['lotion', 'Ross', 'store'],\n",
       " ['expensive', 'Amazon', 'still'],\n",
       " ['love', 'Amazon', 'only'],\n",
       " ['Shower', 'Gel', 'layer'],\n",
       " ['is', 'Shower', 'Gel', 'fragrance'],\n",
       " ['love', 'Body', 'Shop', 'items'],\n",
       " ['get', 'Amazon', 'END'],\n",
       " ['Body', 'Shop'],\n",
       " ['START', 'Amazon', 'price'],\n",
       " ['found', 'Amazon', 'END'],\n",
       " ['sold', 'Amazon.com', 'END'],\n",
       " ['order', 'Amazon', 'cost'],\n",
       " ['salad', 'Whole', 'Foods', 'body'],\n",
       " ['best', 'Axe', 'line'],\n",
       " ['love',\n",
       "  'Clarins',\n",
       "  'Eau',\n",
       "  'Ensoleillante',\n",
       "  'Moisturizing',\n",
       "  'Body',\n",
       "  'Lotion',\n",
       "  'END'],\n",
       " ['disappointed', 'Avon', 'discontinued'],\n",
       " ['items', 'Body', 'shop'],\n",
       " ['shop', 'Bath', 'Body', 'Works', 'END'],\n",
       " ['Thanks', 'Amazon', 'sites'],\n",
       " ['favorite', 'Bath', 'Body', 'Works', 'scent'],\n",
       " ['gels', 'AXE', 'END'],\n",
       " ['acquainted', 'Savannah', 'Bee', 'Mint', 'Julep', 'originally'],\n",
       " ['one', 'Bath', 'Body', 'END'],\n",
       " ['buy', 'Amazon', 'found'],\n",
       " ['awards', 'Pattern', 'Body', 'Wash', 'received'],\n",
       " ['many', 'Boots', 'products'],\n",
       " ['bottle', \"J'Adore\", 'lotion'],\n",
       " ['START', 'Dove', 'no'],\n",
       " ['find', 'Amazon', 'END'],\n",
       " ['product', 'Bath', 'Body', 'one'],\n",
       " ['know', 'Bath', 'Body', 'stopped'],\n",
       " ['find', 'Amazon', 'has'],\n",
       " ['now', 'Amazon', 'not'],\n",
       " ['find', 'Amazon', 'END'],\n",
       " ['years', 'Thymes', 'discontinued'],\n",
       " ['using', 'Vitamin', 'E', 'gel'],\n",
       " ['take', 'Softsoap', 'grain'],\n",
       " ['fan', 'Olay', 'orchid'],\n",
       " ['see', 'Amazon', 'sells'],\n",
       " ['such', 'Sephora', 'other'],\n",
       " ['shipped', 'FPO', 'address'],\n",
       " ['single', 'Escada', 'fragrance'],\n",
       " ['cheaper', 'B&BW', 'END'],\n",
       " ['START', 'Thymes', 'comany'],\n",
       " ['production', 'Filigree', 'glad'],\n",
       " ['day', 'SPA', 'LOVE'],\n",
       " ['using', 'BeautiControl', 'time'],\n",
       " ['fan', 'Molton', 'Brown', 'products'],\n",
       " ['Reviving', 'Cempaka', 'Bath', 'Shower', 'gel'],\n",
       " ['However', 'Tea', 'Tree', 'often'],\n",
       " ['purchased', 'Chanel', 'Antaeus', 'Bath', 'shower'],\n",
       " ['shower', 'Gel', 'END'],\n",
       " ['used', 'Bellmira', 'Herbaflor', 'Herbal', 'Baths', 'years'],\n",
       " ['gel', 'Solstis', 'really'],\n",
       " ['Williams', 'Company', 'ran'],\n",
       " ['is', 'AQUA', 'VELVA', 'MAN', 'seemed'],\n",
       " ['negative', 'Aqua', 'Velva', 'last'],\n",
       " ['have', 'Ambergris', 'quite'],\n",
       " ['one', 'Wal', 'Mart', 'glass'],\n",
       " ['said', 'Waterpik', 'have'],\n",
       " ['floss', 'Oral-B', 'Superfloss', 'END'],\n",
       " ['used', 'Waterpik', 'say'],\n",
       " ['throw', 'Waterpik', 'away'],\n",
       " ['Overall', 'Waterpik', 'lasted'],\n",
       " ['reviews', 'Amazon', 'END'],\n",
       " ['held', 'WP', '450', 'model'],\n",
       " ['use', 'Sonicare', 'END'],\n",
       " ['added', 'Waterpik', 'routine'],\n",
       " ['Transaction', 'Amazon', 'completed'],\n",
       " ['buy', 'Waterpik', 'alternative'],\n",
       " ['floss', 'Waterpik', 'beneficial'],\n",
       " ['START', 'Waterpik', 'easy'],\n",
       " ['START', 'Yugo', 'compared'],\n",
       " ['compared', 'Lexus', 'water'],\n",
       " ['little', 'Godsend', 'also'],\n",
       " ['getting', 'Waterpik', 'instead'],\n",
       " ['use', 'Waterpik', 'END'],\n",
       " ['using', 'Sonicare', 'bought'],\n",
       " ['bought', 'Waterpik', 'obviously'],\n",
       " ['used', 'Waterpik', 'before'],\n",
       " ['put', 'Listerine', 'tank'],\n",
       " ['used', 'Waterpik', 'least'],\n",
       " ['water', 'Listerine', 'worth'],\n",
       " ['first', 'WaterPik', 'came'],\n",
       " ['using', 'WaterPik', 'END'],\n",
       " ['here', 'Amazon', 'eye-opening'],\n",
       " ['made', 'WaterPik', 'original'],\n",
       " ['past', 'WaterPik', 'units'],\n",
       " ['using', 'Listerine', 'END'],\n",
       " ['kinds', 'Crest', 'rinse'],\n",
       " ['kinds', 'Crest', 'toothpaste'],\n",
       " ['version', 'Crest', 'Invigorating'],\n",
       " ['Crest', 'Invigorating', 'Clean', 'Mint'],\n",
       " ['think', 'Crest', 'mouthwashes'],\n",
       " ['sensation', 'Crest', 'Pro-Health', 'mouthwashes'],\n",
       " ['other', 'Crest', 'formulas'],\n",
       " ['similar', 'Scope', 'mouthwash'],\n",
       " ['fine', 'Crest', 'formula'],\n",
       " ['is', 'Crest', 'use'],\n",
       " ['use', 'Yellow', '#', '5', 'food'],\n",
       " ['note', 'Crest', 'used'],\n",
       " ['used', 'Yellow', '#', '6', 'instead'],\n",
       " ['wash', 'Invigorating', 'Clean', 'version'],\n",
       " ['use', 'Yellow', '#', '5', 'used'],\n",
       " ['3', 'Crest', 'Mouthwashes', 'last'],\n",
       " ['START', 'Crest', 'Crest', 'Pro'],\n",
       " ['alternative', 'Listerine', 'Total', 'Care', 'END'],\n",
       " ['use', 'FDA', 'approved'],\n",
       " ['get', 'Crest', 'pro'],\n",
       " ['using', 'Norelco', 'rotary'],\n",
       " ['line', 'Norelco', 'available', 'Arcitec', '1090', 'looked'],\n",
       " ['well', 'Quadra', 'Action', 'SmartTouch'],\n",
       " ['Action', 'SmartTouch', 'XL', 'models'],\n",
       " ['new', 'Philips', 'Norelco', 'drive-'],\n",
       " ['cheaper', 'B&BW', 'END'],\n",
       " ['START', 'Thymes', 'comany'],\n",
       " ['production', 'Filigree', 'glad'],\n",
       " ['here', 'Amazon', 'END'],\n",
       " ['Bvlgari', 'Conditioner', 'favorite'],\n",
       " ['START', 'Selenium', 'appeared'],\n",
       " ['get', 'Amazon', 'END'],\n",
       " ['have', 'Airbnb', 'END'],\n",
       " ['body', 'Spongelle', 'END'],\n",
       " ['prolong', 'Spongelle', 'useful'],\n",
       " ['see', 'P&G', 'bring'],\n",
       " ['favorite', 'Bath', 'Body'],\n",
       " ['Bath', 'Body', 'Works', 'collection'],\n",
       " ['Dave', 'Buster'],\n",
       " ['loss', 'Amazon', 'marketplaces'],\n",
       " ['START', 'Uber', 'pleased', 'D&G', 'Lifht', 'Blue', 'one'],\n",
       " ['pleased', 'Uhuru', 'Naturals', 'Hair'],\n",
       " ['whey', 'Old', 'Spice', 'discontinued'],\n",
       " ['START', 'Google', 'uses'],\n",
       " ['well', 'Northern', 'Fir'],\n",
       " ['START', 'Eau', 'de', 'Hadrien', 'favorite'],\n",
       " ['Tangle', 'Taming', 'Conditioner', 'good'],\n",
       " ['Thanks', 'Amazon', 'no'],\n",
       " ['START', 'Amazon', 'fixed'],\n",
       " ['kinds', 'Dial', 'Soap', 'all'],\n",
       " ['tried', 'Kerastase', 'PHYTO'],\n",
       " ['Kerastase', 'PHYTO', 'Davines'],\n",
       " ['single', 'Coloniali', 'product'],\n",
       " ['par', 'Acqua', 'di', 'Parma', 'best'],\n",
       " ['Thanks', 'Amazon', 'carying'],\n",
       " ['Along', 'Olay', 'Age', 'Defying', 'moisturizer'],\n",
       " ['wish', 'Redken', 'discontinued'],\n",
       " ['Federal',\n",
       "  'Trade',\n",
       "  'Commission',\n",
       "  'Guidelines',\n",
       "  'Testimony',\n",
       "  'Advertising',\n",
       "  'END'],\n",
       " ['ago', 'Sally', 'Beauty', 'Supply', 'END'],\n",
       " ['selling', 'Azur', 'brand'],\n",
       " ['PRELL', 'Conditioner', 'far'],\n",
       " ['find', 'Amazon', 'store'],\n",
       " ['price', 'Amazon', 'went'],\n",
       " ['buy', 'Tree', 'Hut', 'END'],\n",
       " ['company', 'Amazon', 'buy'],\n",
       " ['discontinued', 'Bain', 'De', 'Terre', 'END'],\n",
       " ['used', 'Mousse', 'products'],\n",
       " ['START', 'Wish', 'Walgreens', 'kept'],\n",
       " ['Thank', 'Amazon', 'carrying'],\n",
       " ['use', 'Pureology', 'END'],\n",
       " ['product', 'Pantene', 'made'],\n",
       " ['START', 'Bath', 'Body'],\n",
       " ['Bath', 'Body', 'Works', 'quit'],\n",
       " ['sold', 'Amazon', 'END'],\n",
       " ['START', 'Monoi', 'Tiare', 'Tahiti', 'Coconut', 'Oil'],\n",
       " ['has', 'Tiare', 'flower'],\n",
       " ['scent', 'Bath', 'Body', 'Works', 'formula'],\n",
       " ['well', 'Vitamin', 'E', 'actual'],\n",
       " ['thing', 'B&BW', 'formula'],\n",
       " ['START', 'Perlier', 'makes'],\n",
       " ['bought', 'Big', 'Lots', 'only'],\n",
       " ['smell', 'Dial', 'Mountain', 'Fresh', 'also'],\n",
       " ['promotes', 'Antibacterial', 'Wash', 'END'],\n",
       " ['bought', 'Channel', '5', 'thinking'],\n",
       " ['like', 'H20', 'shampoo'],\n",
       " ['down', 'Disney', 'World', 'END'],\n",
       " ['loved', 'Disney', 'world'],\n",
       " ['START', 'HALE', 'BERRY', 'Pure', 'Orchid', 'wonderful'],\n",
       " ['goodness', 'Amazon', 'END'],\n",
       " ['received', 'Popsugar', 'box'],\n",
       " ['using', 'Calibra', 'eye'],\n",
       " ['looked', 'Amazon', ';-)'],\n",
       " ['business', 'Amazon', 'END'],\n",
       " ['found', 'Amazon', 'END'],\n",
       " ['START', 'Fragarant', 'strong'],\n",
       " ['Mother', 'Christmas', 'loves'],\n",
       " ['using', 'Neutrogena', 'remove'],\n",
       " ['went', 'Amazon', 'ordered'],\n",
       " ['kit', 'Bare', 'Escentuals', 'END'],\n",
       " ['price', 'Hermes', 'product'],\n",
       " ['START', 'Henkel', 'quit'],\n",
       " ['worth', 'Amazon', 'no'],\n",
       " ['favorite', 'Thymes', 'collection'],\n",
       " ['START', 'Azur', 'discontinued'],\n",
       " ['product', 'Sally', 'Beauty', 'Supply', 'END'],\n",
       " ['have', 'Green', 'Tea', 'cologne', 'Green', 'Tea', 'Body'],\n",
       " ['same', 'Green', 'Tea', 'END'],\n",
       " ['original', 'Creme', 'Perfume', 'tube', 'Body', 'Lotion', 'END'],\n",
       " ['travel', 'TSA', 'regulations'],\n",
       " ['making', '32oz', 'END'],\n",
       " ['disappointed', 'BedBathandBeyond', 'discontinued'],\n",
       " ['find', 'Amazon', 'END'],\n",
       " ['buy', 'Moon', 'Sparkle', 'again'],\n",
       " ['lotion', 'Ross', 'store'],\n",
       " ['expensive', 'Amazon', 'still'],\n",
       " ['love', 'Amazon', 'only'],\n",
       " ['Shower', 'Gel', 'layer'],\n",
       " ['is', 'Shower', 'Gel', 'fragrance'],\n",
       " ['love', 'Body', 'Shop', 'items'],\n",
       " ['get', 'Amazon', 'END'],\n",
       " ['well', 'Bare', 'Minerals', 'loose'],\n",
       " ['Body', 'Shop'],\n",
       " ['original',\n",
       "  'Matrix',\n",
       "  'Amplifying',\n",
       "  '/',\n",
       "  'Volumizing',\n",
       "  'System',\n",
       "  'Shampoo',\n",
       "  'number'],\n",
       " ['better', 'Matrix', 'new'],\n",
       " ['START', 'Matrix', 'never'],\n",
       " ['START', 'Amazon', 'price'],\n",
       " ['found', 'Amazon', 'END'],\n",
       " ['sold', 'Amazon.com', 'END'],\n",
       " ['order', 'Amazon', 'cost'],\n",
       " ['shampoo', 'Detangler', 'Conditioner'],\n",
       " ['local', 'Sally', 'Beauty', 'Supply', 'grocery'],\n",
       " ['use', 'Estee', 'Lauder', 'products'],\n",
       " ['products', 'Amnazon', 'great'],\n",
       " ['salad', 'Whole', 'Foods', 'body'],\n",
       " ['best', 'Axe', 'line'],\n",
       " ['love',\n",
       "  'Clarins',\n",
       "  'Eau',\n",
       "  'Ensoleillante',\n",
       "  'Moisturizing',\n",
       "  'Body',\n",
       "  'Lotion',\n",
       "  'END'],\n",
       " ['disappointed', 'Avon', 'discontinued'],\n",
       " ['items', 'Body', 'shop'],\n",
       " ['shop', 'Bath', 'Body', 'Works', 'END'],\n",
       " ['Thanks', 'Amazon', 'sites'],\n",
       " ['favorite', 'Bath', 'Body', 'Works', 'scent'],\n",
       " ['use', 'Estee', 'Lauder', 'foundation'],\n",
       " ['together', 'MOP', 'c-straight'],\n",
       " ['use', 'Matrix', 'conditioner'],\n",
       " ['gels', 'AXE', 'END'],\n",
       " ['START', 'Matrix', 'Biolage', 'Fortifying', 'conditioner'],\n",
       " ['acquainted', 'Savannah', 'Bee', 'Mint', 'Julep', 'originally'],\n",
       " ['one', 'Bath', 'Body', 'END'],\n",
       " ['buy', 'Amazon', 'found'],\n",
       " ['awards', 'Pattern', 'Body', 'Wash', 'received'],\n",
       " ['worn', 'Chanel', '#', '5', '12'],\n",
       " ['many', 'Boots', 'products'],\n",
       " ['bottle', \"J'Adore\", 'lotion'],\n",
       " ['always', 'Bare', 'Minerals', 'never'],\n",
       " ['START', 'Dove', 'no'],\n",
       " ['find', 'Amazon', 'END'],\n",
       " ['product', 'Bath', 'Body', 'one'],\n",
       " ['know', 'Bath', 'Body', 'stopped'],\n",
       " ['find', 'Amazon', 'has'],\n",
       " ['now', 'Amazon', 'not'],\n",
       " ['find', 'Amazon', 'END'],\n",
       " ['years', 'Thymes', 'discontinued'],\n",
       " ['recommended', 'HBL', 'products'],\n",
       " ['using', 'HBL', 'volumizing'],\n",
       " ['Shine', 'Seal', 'END'],\n",
       " ['using', 'Vitamin', 'E', 'gel'],\n",
       " ['Reflection', 'Chroma', 'Thermique', 'also'],\n",
       " ['take', 'Softsoap', 'grain'],\n",
       " ['fan', 'Olay', 'orchid'],\n",
       " ['see', 'Amazon', 'sells'],\n",
       " ['used', 'Biolage', 'products'],\n",
       " ['such', 'Sephora', 'other'],\n",
       " ['shipped', 'FPO', 'address'],\n",
       " ['single', 'Escada', 'fragrance'],\n",
       " ['consider', 'Macallan', 'single'],\n",
       " ['again', 'Amazon', 'thanks'],\n",
       " ['here', 'Sonicare', 'produce'],\n",
       " ['have', 'Sonicare', 'toothbrush'],\n",
       " ['heads', 'Oral', 'B-', 'FlexiSoft', 'FlossAction'],\n",
       " ['FlexiSoft', 'FlossAction', 'most'],\n",
       " ['most', 'Sonicare', 'models'],\n",
       " ['holder', 'Sanitizer', 'END'],\n",
       " ['using', 'Essie', 'product'],\n",
       " ['adherence', 'FTC', 'regulations'],\n",
       " ['10.5', 'Womens', 'END'],\n",
       " ['brush', 'Urban', 'Spa', 'END'],\n",
       " ['Urban', 'Spa', 'Loofah', 'try'],\n",
       " ['Urban', 'Spa', 'Microfiber', 'Bath', 'Pillow', 'makes'],\n",
       " ['pillow', 'Amazon', 'has'],\n",
       " ['points', 'QC', 'issue'],\n",
       " ['START', 'Urban', 'Spa', 'Get'],\n",
       " ['Get', 'Grip', 'Trio', 'Manicure', 'Pedicure'],\n",
       " ['START', 'Avalon', 'Organics', 'has'],\n",
       " ['Water', 'Stearic', 'Acid', 'Myristic'],\n",
       " ['Acid', 'Myristic', 'Acid', 'Squalane'],\n",
       " ['Acid', 'Squalane', 'Glycerin'],\n",
       " ['Squalane', 'Glycerin', 'Potassium'],\n",
       " ['Glycerin', 'Potassium', 'Hydroxide', 'Organic'],\n",
       " ['Hydroxide', 'Organic', 'Coconut', 'Oil', 'Glycol'],\n",
       " ['Oil', 'Glycol', 'Distearate', 'Fragrance'],\n",
       " ['Distearate', 'Fragrance', 'Organic'],\n",
       " ['Fragrance',\n",
       "  'Organic',\n",
       "  'Jojoba',\n",
       "  'Seed',\n",
       "  'Oil',\n",
       "  'Organic',\n",
       "  'Pomegranate',\n",
       "  'Oil',\n",
       "  'Organic',\n",
       "  'Green',\n",
       "  'Tea',\n",
       "  'Leaf',\n",
       "  'Extract',\n",
       "  'Organic',\n",
       "  'Shea',\n",
       "  'Butter',\n",
       "  'Fruit',\n",
       "  'Phenoxyethanol'],\n",
       " ['Fruit', 'Phenoxyethanol', 'Tetrahydroxypropyl'],\n",
       " ['Phenoxyethanol', 'Tetrahydroxypropyl', 'Ethylenediamine', 'Ethylhexyl'],\n",
       " ['Ethylenediamine', 'Ethylhexyl', 'Octyl'],\n",
       " ['Ethylhexyl', 'Octyl', 'Palmitate', 'Ethylhexylglycerin'],\n",
       " ['Palmitate', 'Ethylhexylglycerin', 'Tocopherol'],\n",
       " ['Ethylhexylglycerin', 'Tocopherol', 'Limonene'],\n",
       " ['Tocopherol', 'Limonene', 'END'],\n",
       " ['used', 'Bellmira', 'Herbaflor', 'Herbal', 'Baths', 'years'],\n",
       " ['START', 'Selenium', 'appeared'],\n",
       " ['get', 'Amazon', 'END'],\n",
       " ['have', 'Airbnb', 'END'],\n",
       " ['body', 'Spongelle', 'END'],\n",
       " ['prolong', 'Spongelle', 'useful'],\n",
       " ['see', 'P&G', 'bring'],\n",
       " ['favorite', 'Bath', 'Body'],\n",
       " ['Bath', 'Body', 'Works', 'collection'],\n",
       " ['Dave', 'Buster'],\n",
       " ['loss', 'Amazon', 'marketplaces'],\n",
       " ['START', 'Uber', 'pleased', 'D&G', 'Lifht', 'Blue', 'one'],\n",
       " ['pleased', 'Uhuru', 'Naturals', 'Hair'],\n",
       " ['whey', 'Old', 'Spice', 'discontinued'],\n",
       " ['START', 'Google', 'uses'],\n",
       " ['well', 'Northern', 'Fir'],\n",
       " ['START', 'Eau', 'de', 'Hadrien', 'favorite'],\n",
       " ['Tangle', 'Taming', 'Conditioner', 'good'],\n",
       " ['Thanks', 'Amazon', 'no'],\n",
       " ['START', 'Amazon', 'fixed'],\n",
       " ['kinds', 'Dial', 'Soap', 'all'],\n",
       " ['tried', 'Kerastase', 'PHYTO'],\n",
       " ['Kerastase', 'PHYTO', 'Davines'],\n",
       " ['single', 'Coloniali', 'product'],\n",
       " ['par', 'Acqua', 'di', 'Parma', 'best'],\n",
       " ['Thanks', 'Amazon', 'carying'],\n",
       " ['Along', 'Olay', 'Age', 'Defying', 'moisturizer'],\n",
       " ['wish', 'Redken', 'discontinued'],\n",
       " ['Federal',\n",
       "  'Trade',\n",
       "  'Commission',\n",
       "  'Guidelines',\n",
       "  'Testimony',\n",
       "  'Advertising',\n",
       "  'END'],\n",
       " ['ago', 'Sally', 'Beauty', 'Supply', 'END'],\n",
       " ['selling', 'Azur', 'brand'],\n",
       " ['PRELL', 'Conditioner', 'far'],\n",
       " ['find', 'Amazon', 'store'],\n",
       " ['price', 'Amazon', 'went'],\n",
       " ['buy', 'Tree', 'Hut', 'END'],\n",
       " ['company', 'Amazon', 'buy'],\n",
       " ['discontinued', 'Bain', 'De', 'Terre', 'END'],\n",
       " ['used', 'Mousse', 'products'],\n",
       " ['START', 'Wish', 'Walgreens', 'kept'],\n",
       " ['Thank', 'Amazon', 'carrying'],\n",
       " ['use', 'Pureology', 'END'],\n",
       " ['product', 'Pantene', 'made'],\n",
       " ['START', 'Bath', 'Body'],\n",
       " ['Bath', 'Body', 'Works', 'quit'],\n",
       " ['sold', 'Amazon', 'END'],\n",
       " ['START', 'Monoi', 'Tiare', 'Tahiti', 'Coconut', 'Oil'],\n",
       " ['has', 'Tiare', 'flower'],\n",
       " ['scent', 'Bath', 'Body', 'Works', 'formula'],\n",
       " ['well', 'Vitamin', 'E', 'actual'],\n",
       " ['thing', 'B&BW', 'formula'],\n",
       " ['START', 'Perlier', 'makes'],\n",
       " ['bought', 'Big', 'Lots', 'only'],\n",
       " ['smell', 'Dial', 'Mountain', 'Fresh', 'also'],\n",
       " ['promotes', 'Antibacterial', 'Wash', 'END'],\n",
       " ['bought', 'Channel', '5', 'thinking'],\n",
       " ['like', 'H20', 'shampoo'],\n",
       " ['down', 'Disney', 'World', 'END'],\n",
       " ['loved', 'Disney', 'world'],\n",
       " ['START', 'HALE', 'BERRY', 'Pure', 'Orchid', 'wonderful'],\n",
       " ['goodness', 'Amazon', 'END'],\n",
       " ['received', 'Popsugar', 'box'],\n",
       " ['using', 'Calibra', 'eye'],\n",
       " ['looked', 'Amazon', ';-)'],\n",
       " ['business', 'Amazon', 'END'],\n",
       " ['found', 'Amazon', 'END'],\n",
       " ['START', 'Fragarant', 'strong'],\n",
       " ['Mother', 'Christmas', 'loves'],\n",
       " ['using', 'Neutrogena', 'remove'],\n",
       " ['went', 'Amazon', 'ordered'],\n",
       " ['kit', 'Bare', 'Escentuals', 'END'],\n",
       " ['price', 'Hermes', 'product'],\n",
       " ['START', 'Henkel', 'quit'],\n",
       " ['worth', 'Amazon', 'no'],\n",
       " ['favorite', 'Thymes', 'collection'],\n",
       " ['START', 'Azur', 'discontinued'],\n",
       " ['product', 'Sally', 'Beauty', 'Supply', 'END'],\n",
       " ['have', 'Green', 'Tea', 'cologne', 'Green', 'Tea', 'Body'],\n",
       " ['same', 'Green', 'Tea', 'END'],\n",
       " ['original', 'Creme', 'Perfume', 'tube', 'Body', 'Lotion', 'END'],\n",
       " ['travel', 'TSA', 'regulations'],\n",
       " ['making', '32oz', 'END'],\n",
       " ['disappointed', 'BedBathandBeyond', 'discontinued'],\n",
       " ['find', 'Amazon', 'END'],\n",
       " ['buy', 'Moon', 'Sparkle', 'again'],\n",
       " ['lotion', 'Ross', 'store'],\n",
       " ['expensive', 'Amazon', 'still'],\n",
       " ['love', 'Amazon', 'only'],\n",
       " ['Shower', 'Gel', 'layer'],\n",
       " ['is', 'Shower', 'Gel', 'fragrance'],\n",
       " ['love', 'Body', 'Shop', 'items'],\n",
       " ['get', 'Amazon', 'END'],\n",
       " ['well', 'Bare', 'Minerals', 'loose'],\n",
       " ['Body', 'Shop'],\n",
       " ['original',\n",
       "  'Matrix',\n",
       "  'Amplifying',\n",
       "  '/',\n",
       "  'Volumizing',\n",
       "  'System',\n",
       "  'Shampoo',\n",
       "  'number'],\n",
       " ['better', 'Matrix', 'new'],\n",
       " ['START', 'Matrix', 'never'],\n",
       " ['START', 'Amazon', 'price'],\n",
       " ['found', 'Amazon', 'END'],\n",
       " ['sold', 'Amazon.com', 'END'],\n",
       " ['order', 'Amazon', 'cost'],\n",
       " ['shampoo', 'Detangler', 'Conditioner'],\n",
       " ['local', 'Sally', 'Beauty', 'Supply', 'grocery'],\n",
       " ['use', 'Estee', 'Lauder', 'products'],\n",
       " ['products', 'Amnazon', 'great'],\n",
       " ['salad', 'Whole', 'Foods', 'body'],\n",
       " ['best', 'Axe', 'line'],\n",
       " ['love',\n",
       "  'Clarins',\n",
       "  'Eau',\n",
       "  'Ensoleillante',\n",
       "  'Moisturizing',\n",
       "  'Body',\n",
       "  'Lotion',\n",
       "  'END'],\n",
       " ['disappointed', 'Avon', 'discontinued'],\n",
       " ['items', 'Body', 'shop'],\n",
       " ['shop', 'Bath', 'Body', 'Works', 'END'],\n",
       " ['Thanks', 'Amazon', 'sites'],\n",
       " ['favorite', 'Bath', 'Body', 'Works', 'scent'],\n",
       " ['use', 'Estee', 'Lauder', 'foundation'],\n",
       " ['together', 'MOP', 'c-straight'],\n",
       " ['use', 'Matrix', 'conditioner'],\n",
       " ['gels', 'AXE', 'END'],\n",
       " ['START', 'Matrix', 'Biolage', 'Fortifying', 'conditioner'],\n",
       " ['acquainted', 'Savannah', 'Bee', 'Mint', 'Julep', 'originally'],\n",
       " ['one', 'Bath', 'Body', 'END'],\n",
       " ['buy', 'Amazon', 'found'],\n",
       " ['awards', 'Pattern', 'Body', 'Wash', 'received'],\n",
       " ['worn', 'Chanel', '#', '5', '12'],\n",
       " ['many', 'Boots', 'products'],\n",
       " ['bottle', \"J'Adore\", 'lotion'],\n",
       " ['always', 'Bare', 'Minerals', 'never'],\n",
       " ['START', 'Dove', 'no'],\n",
       " ['find', 'Amazon', 'END'],\n",
       " ['product', 'Bath', 'Body', 'one'],\n",
       " ['know', 'Bath', 'Body', 'stopped'],\n",
       " ['find', 'Amazon', 'has'],\n",
       " ['now', 'Amazon', 'not'],\n",
       " ['find', 'Amazon', 'END'],\n",
       " ['years', 'Thymes', 'discontinued'],\n",
       " ['recommended', 'HBL', 'products'],\n",
       " ['using', 'HBL', 'volumizing'],\n",
       " ['Shine', 'Seal', 'END'],\n",
       " ['using', 'Vitamin', 'E', 'gel'],\n",
       " ['Reflection', 'Chroma', 'Thermique', 'also'],\n",
       " ['take', 'Softsoap', 'grain'],\n",
       " ['fan', 'Olay', 'orchid'],\n",
       " ['see', 'Amazon', 'sells'],\n",
       " ['used', 'Biolage', 'products'],\n",
       " ['such', 'Sephora', 'other'],\n",
       " ['shipped', 'FPO', 'address'],\n",
       " ['single', 'Escada', 'fragrance'],\n",
       " ['consider', 'Macallan', 'single'],\n",
       " ['again', 'Amazon', 'thanks'],\n",
       " ['cheaper', 'B&BW', 'END'],\n",
       " ['START', 'Thymes', 'comany'],\n",
       " ['production', 'Filigree', 'glad'],\n",
       " ['here', 'Amazon', 'END'],\n",
       " ['Bvlgari', 'Conditioner', 'favorite'],\n",
       " ['stayed', 'Ritz-Carlton', 'Mexico'],\n",
       " ['buy', 'Kerastase', 'Fusio-', 'Dose', 'Systeme', 'Professionnel', 'END'],\n",
       " ['day', 'SPA', 'LOVE'],\n",
       " ['using', 'BeautiControl', 'time'],\n",
       " ['fan', 'Molton', 'Brown', 'products'],\n",
       " ['Reviving', 'Cempaka', 'Bath', 'Shower', 'gel'],\n",
       " ['However', 'Tea', 'Tree', 'often'],\n",
       " ['purchased', 'Chanel', 'Antaeus', 'Bath', 'shower'],\n",
       " ['shower', 'Gel', 'END'],\n",
       " ['Sea', 'Salt', 'Kelp', 'Winter', 'Winter', 'Spice', 'best'],\n",
       " ['review', 'Sea', 'Salt', 'w', 'Kelp', 'soap'],\n",
       " ['buy', 'Kerastase', 'Fusio-', 'Dose', 'Systeme', 'Professionnel', 'END'],\n",
       " ['day', 'SPA', 'LOVE'],\n",
       " ['using', 'BeautiControl', 'time'],\n",
       " ['fan', 'Molton', 'Brown', 'products'],\n",
       " ['Reviving', 'Cempaka', 'Bath', 'Shower', 'gel'],\n",
       " ['However', 'Tea', 'Tree', 'often'],\n",
       " ['purchased', 'Chanel', 'Antaeus', 'Bath', 'shower'],\n",
       " ['shower', 'Gel', 'END'],\n",
       " ['Sea', 'Salt', 'Kelp', 'Winter', 'Winter', 'Spice', 'best'],\n",
       " ['review', 'Sea', 'Salt', 'w', 'Kelp', 'soap'],\n",
       " ['used', 'Bellmira', 'Herbaflor', 'Herbal', 'Baths', 'years'],\n",
       " ['Thanks', 'Combe', 'Inc', 'END'],\n",
       " ['Lectric', 'Shave', 'END'],\n",
       " ['added', 'Lectric', 'Shave', 'green'],\n",
       " ['turned', 'Amazon', 'END'],\n",
       " ['used', 'Lectric', 'Shave', 'decades'],\n",
       " ['looking', 'Lectric', 'Shave', 'END'],\n",
       " ['used', 'Lectric', 'Shave', '1973'],\n",
       " ['1973', 'Mark', '4', 'Remington', 'few'],\n",
       " ['store', \"Norelco's\", 'one'],\n",
       " ['horrible', 'Sunbeam', 'now'],\n",
       " ['favorite', 'Braun', '320s'],\n",
       " ['Braun', '320s', '4', '#'],\n",
       " ['ms-3/4700', 'Remington', 'micro'],\n",
       " ['razor', 'Rem', 'ever'],\n",
       " ['great', 'Military', 'base'],\n",
       " ['Axe',\n",
       "  'Detailer',\n",
       "  'Shower',\n",
       "  'Tool',\n",
       "  'basically',\n",
       "  'Ax',\n",
       "  'Shower',\n",
       "  'Tool',\n",
       "  'get'],\n",
       " ['using', 'Essie', 'product'],\n",
       " ['adherence', 'FTC', 'regulations'],\n",
       " ['10.5', 'Womens', 'END'],\n",
       " ['brush', 'Urban', 'Spa', 'END'],\n",
       " ['Urban', 'Spa', 'Loofah', 'try'],\n",
       " ['Urban', 'Spa', 'Microfiber', 'Bath', 'Pillow', 'makes'],\n",
       " ['pillow', 'Amazon', 'has'],\n",
       " ['points', 'QC', 'issue'],\n",
       " ['START', 'Urban', 'Spa', 'Get'],\n",
       " ['Get', 'Grip', 'Trio', 'Manicure', 'Pedicure'],\n",
       " ['START', 'Avalon', 'Organics', 'has'],\n",
       " ['Water', 'Stearic', 'Acid', 'Myristic'],\n",
       " ['Acid', 'Myristic', 'Acid', 'Squalane'],\n",
       " ['Acid', 'Squalane', 'Glycerin'],\n",
       " ['Squalane', 'Glycerin', 'Potassium'],\n",
       " ['Glycerin', 'Potassium', 'Hydroxide', 'Organic'],\n",
       " ['Hydroxide', 'Organic', 'Coconut', 'Oil', 'Glycol'],\n",
       " ['Oil', 'Glycol', 'Distearate', 'Fragrance'],\n",
       " ['Distearate', 'Fragrance', 'Organic'],\n",
       " ['Fragrance',\n",
       "  'Organic',\n",
       "  'Jojoba',\n",
       "  'Seed',\n",
       "  'Oil',\n",
       "  'Organic',\n",
       "  'Pomegranate',\n",
       "  'Oil',\n",
       "  'Organic',\n",
       "  'Green',\n",
       "  'Tea',\n",
       "  'Leaf',\n",
       "  'Extract',\n",
       "  'Organic',\n",
       "  'Shea',\n",
       "  'Butter',\n",
       "  'Fruit',\n",
       "  'Phenoxyethanol'],\n",
       " ['Fruit', 'Phenoxyethanol', 'Tetrahydroxypropyl'],\n",
       " ['Phenoxyethanol', 'Tetrahydroxypropyl', 'Ethylenediamine', 'Ethylhexyl'],\n",
       " ['Ethylenediamine', 'Ethylhexyl', 'Octyl'],\n",
       " ['Ethylhexyl', 'Octyl', 'Palmitate', 'Ethylhexylglycerin'],\n",
       " ['Palmitate', 'Ethylhexylglycerin', 'Tocopherol'],\n",
       " ['Ethylhexylglycerin', 'Tocopherol', 'Limonene'],\n",
       " ['Tocopherol', 'Limonene', 'END'],\n",
       " ['used', 'Bellmira', 'Herbaflor', 'Herbal', 'Baths', 'years'],\n",
       " ['START', 'Selenium', 'appeared'],\n",
       " ['get', 'Amazon', 'END'],\n",
       " ['have', 'Airbnb', 'END'],\n",
       " ['body', 'Spongelle', 'END'],\n",
       " ['prolong', 'Spongelle', 'useful'],\n",
       " ['see', 'P&G', 'bring'],\n",
       " ['favorite', 'Bath', 'Body'],\n",
       " ['Bath', 'Body', 'Works', 'collection'],\n",
       " ['Dave', 'Buster'],\n",
       " ['loss', 'Amazon', 'marketplaces'],\n",
       " ['START', 'Uber', 'pleased', 'D&G', 'Lifht', 'Blue', 'one'],\n",
       " ['pleased', 'Uhuru', 'Naturals', 'Hair'],\n",
       " ['whey', 'Old', 'Spice', 'discontinued'],\n",
       " ['START', 'Google', 'uses'],\n",
       " ['well', 'Northern', 'Fir'],\n",
       " ['START', 'Eau', 'de', 'Hadrien', 'favorite'],\n",
       " ['Tangle', 'Taming', 'Conditioner', 'good'],\n",
       " ['Thanks', 'Amazon', 'no'],\n",
       " ['START', 'Amazon', 'fixed'],\n",
       " ['kinds', 'Dial', 'Soap', 'all'],\n",
       " ['tried', 'Kerastase', 'PHYTO'],\n",
       " ['Kerastase', 'PHYTO', 'Davines'],\n",
       " ['single', 'Coloniali', 'product'],\n",
       " ['par', 'Acqua', 'di', 'Parma', 'best'],\n",
       " ['Thanks', 'Amazon', 'carying'],\n",
       " ['Along', 'Olay', 'Age', 'Defying', 'moisturizer'],\n",
       " ['wish', 'Redken', 'discontinued'],\n",
       " ['Federal',\n",
       "  'Trade',\n",
       "  'Commission',\n",
       "  'Guidelines',\n",
       "  'Testimony',\n",
       "  'Advertising',\n",
       "  'END'],\n",
       " ['ago', 'Sally', 'Beauty', 'Supply', 'END'],\n",
       " ['selling', 'Azur', 'brand'],\n",
       " ['PRELL', 'Conditioner', 'far'],\n",
       " ['find', 'Amazon', 'store'],\n",
       " ['price', 'Amazon', 'went'],\n",
       " ['buy', 'Tree', 'Hut', 'END'],\n",
       " ['company', 'Amazon', 'buy'],\n",
       " ['discontinued', 'Bain', 'De', 'Terre', 'END'],\n",
       " ['used', 'Mousse', 'products'],\n",
       " ['START', 'Wish', 'Walgreens', 'kept'],\n",
       " ['Thank', 'Amazon', 'carrying'],\n",
       " ['use', 'Pureology', 'END'],\n",
       " ['product', 'Pantene', 'made'],\n",
       " ['START', 'Bath', 'Body'],\n",
       " ['Bath', 'Body', 'Works', 'quit'],\n",
       " ['sold', 'Amazon', 'END'],\n",
       " ['START', 'Monoi', 'Tiare', 'Tahiti', 'Coconut', 'Oil'],\n",
       " ['has', 'Tiare', 'flower'],\n",
       " ['scent', 'Bath', 'Body', 'Works', 'formula'],\n",
       " ['well', 'Vitamin', 'E', 'actual'],\n",
       " ['thing', 'B&BW', 'formula'],\n",
       " ['START', 'Perlier', 'makes'],\n",
       " ['bought', 'Big', 'Lots', 'only'],\n",
       " ['smell', 'Dial', 'Mountain', 'Fresh', 'also'],\n",
       " ['promotes', 'Antibacterial', 'Wash', 'END'],\n",
       " ['bought', 'Channel', '5', 'thinking'],\n",
       " ['like', 'H20', 'shampoo'],\n",
       " ['down', 'Disney', 'World', 'END'],\n",
       " ['loved', 'Disney', 'world'],\n",
       " ['START', 'HALE', 'BERRY', 'Pure', 'Orchid', 'wonderful'],\n",
       " ['goodness', 'Amazon', 'END'],\n",
       " ['received', 'Popsugar', 'box'],\n",
       " ['using', 'Calibra', 'eye'],\n",
       " ['looked', 'Amazon', ';-)'],\n",
       " ['business', 'Amazon', 'END'],\n",
       " ['found', 'Amazon', 'END'],\n",
       " ['START', 'Fragarant', 'strong'],\n",
       " ['Mother', 'Christmas', 'loves'],\n",
       " ['using', 'Neutrogena', 'remove'],\n",
       " ['went', 'Amazon', 'ordered'],\n",
       " ['kit', 'Bare', 'Escentuals', 'END'],\n",
       " ['price', 'Hermes', 'product'],\n",
       " ['START', 'Henkel', 'quit'],\n",
       " ['worth', 'Amazon', 'no'],\n",
       " ['favorite', 'Thymes', 'collection'],\n",
       " ['START', 'Azur', 'discontinued'],\n",
       " ['product', 'Sally', 'Beauty', 'Supply', 'END'],\n",
       " ['have', 'Green', 'Tea', 'cologne', 'Green', 'Tea', 'Body'],\n",
       " ['same', 'Green', 'Tea', 'END'],\n",
       " ['original', 'Creme', 'Perfume', 'tube', 'Body', 'Lotion', 'END'],\n",
       " ['travel', 'TSA', 'regulations'],\n",
       " ['making', '32oz', 'END'],\n",
       " ['disappointed', 'BedBathandBeyond', 'discontinued'],\n",
       " ['find', 'Amazon', 'END'],\n",
       " ['buy', 'Moon', 'Sparkle', 'again'],\n",
       " ['lotion', 'Ross', 'store'],\n",
       " ['expensive', 'Amazon', 'still'],\n",
       " ['love', 'Amazon', 'only'],\n",
       " ['Shower', 'Gel', 'layer'],\n",
       " ['is', 'Shower', 'Gel', 'fragrance'],\n",
       " ['love', 'Body', 'Shop', 'items'],\n",
       " ['get', 'Amazon', 'END'],\n",
       " ['well', 'Bare', 'Minerals', 'loose'],\n",
       " ['Body', 'Shop'],\n",
       " ['original',\n",
       "  'Matrix',\n",
       "  'Amplifying',\n",
       "  '/',\n",
       "  'Volumizing',\n",
       "  'System',\n",
       "  'Shampoo',\n",
       "  'number'],\n",
       " ['better', 'Matrix', 'new'],\n",
       " ['START', 'Matrix', 'never'],\n",
       " ['START', 'Amazon', 'price'],\n",
       " ['found', 'Amazon', 'END'],\n",
       " ['sold', 'Amazon.com', 'END'],\n",
       " ['order', 'Amazon', 'cost'],\n",
       " ['shampoo', 'Detangler', 'Conditioner'],\n",
       " ['local', 'Sally', 'Beauty', 'Supply', 'grocery'],\n",
       " ['use', 'Estee', 'Lauder', 'products'],\n",
       " ['products', 'Amnazon', 'great'],\n",
       " ['salad', 'Whole', 'Foods', 'body'],\n",
       " ['best', 'Axe', 'line'],\n",
       " ['love',\n",
       "  'Clarins',\n",
       "  'Eau',\n",
       "  'Ensoleillante',\n",
       "  'Moisturizing',\n",
       "  'Body',\n",
       "  'Lotion',\n",
       "  'END'],\n",
       " ['disappointed', 'Avon', 'discontinued'],\n",
       " ['items', 'Body', 'shop'],\n",
       " ['shop', 'Bath', 'Body', 'Works', 'END'],\n",
       " ['Thanks', 'Amazon', 'sites'],\n",
       " ['favorite', 'Bath', 'Body', 'Works', 'scent'],\n",
       " ['use', 'Estee', 'Lauder', 'foundation'],\n",
       " ['together', 'MOP', 'c-straight'],\n",
       " ['use', 'Matrix', 'conditioner'],\n",
       " ['gels', 'AXE', 'END'],\n",
       " ['START', 'Matrix', 'Biolage', 'Fortifying', 'conditioner'],\n",
       " ['acquainted', 'Savannah', 'Bee', 'Mint', 'Julep', 'originally'],\n",
       " ['one', 'Bath', 'Body', 'END'],\n",
       " ['buy', 'Amazon', 'found'],\n",
       " ['awards', 'Pattern', 'Body', 'Wash', 'received'],\n",
       " ['worn', 'Chanel', '#', '5', '12'],\n",
       " ['many', 'Boots', 'products'],\n",
       " ['bottle', \"J'Adore\", 'lotion'],\n",
       " ['always', 'Bare', 'Minerals', 'never'],\n",
       " ['START', 'Dove', 'no'],\n",
       " ['find', 'Amazon', 'END'],\n",
       " ['product', 'Bath', 'Body', 'one'],\n",
       " ['know', 'Bath', 'Body', 'stopped'],\n",
       " ['find', 'Amazon', 'has'],\n",
       " ['now', 'Amazon', 'not'],\n",
       " ['find', 'Amazon', 'END'],\n",
       " ['years', 'Thymes', 'discontinued'],\n",
       " ['recommended', 'HBL', 'products'],\n",
       " ['using', 'HBL', 'volumizing'],\n",
       " ['Shine', 'Seal', 'END'],\n",
       " ['using', 'Vitamin', 'E', 'gel'],\n",
       " ['Reflection', 'Chroma', 'Thermique', 'also'],\n",
       " ['take', 'Softsoap', 'grain'],\n",
       " ['fan', 'Olay', 'orchid'],\n",
       " ['see', 'Amazon', 'sells'],\n",
       " ['used', 'Biolage', 'products'],\n",
       " ['such', 'Sephora', 'other'],\n",
       " ['shipped', 'FPO', 'address'],\n",
       " ['single', 'Escada', 'fragrance'],\n",
       " ['consider', 'Macallan', 'single'],\n",
       " ['again', 'Amazon', 'thanks'],\n",
       " ['cheaper', 'B&BW', 'END'],\n",
       " ['START', 'Thymes', 'comany'],\n",
       " ['production', 'Filigree', 'glad'],\n",
       " ['here', 'Amazon', 'END'],\n",
       " ['Bvlgari', 'Conditioner', 'favorite'],\n",
       " ['stayed', 'Ritz-Carlton', 'Mexico'],\n",
       " ['buy', 'Kerastase', 'Fusio-', 'Dose', 'Systeme', 'Professionnel', 'END'],\n",
       " ['day', 'SPA', 'LOVE'],\n",
       " ['using', 'BeautiControl', 'time'],\n",
       " ['fan', 'Molton', 'Brown', 'products'],\n",
       " ['Reviving', 'Cempaka', 'Bath', 'Shower', 'gel'],\n",
       " ['However', 'Tea', 'Tree', 'often'],\n",
       " ['purchased', 'Chanel', 'Antaeus', 'Bath', 'shower'],\n",
       " ['shower', 'Gel', 'END'],\n",
       " ['Sea', 'Salt', 'Kelp', 'Winter', 'Winter', 'Spice', 'best'],\n",
       " ['review', 'Sea', 'Salt', 'w', 'Kelp', 'soap'],\n",
       " ['buy', 'Kerastase', 'Fusio-', 'Dose', 'Systeme', 'Professionnel', 'END'],\n",
       " ['day', 'SPA', 'LOVE'],\n",
       " ['using', 'BeautiControl', 'time'],\n",
       " ['fan', 'Molton', 'Brown', 'products'],\n",
       " ['Reviving', 'Cempaka', 'Bath', 'Shower', 'gel'],\n",
       " ['However', 'Tea', 'Tree', 'often'],\n",
       " ['purchased', 'Chanel', 'Antaeus', 'Bath', 'shower'],\n",
       " ['shower', 'Gel', 'END'],\n",
       " ['Sea', 'Salt', 'Kelp', 'Winter', 'Winter', 'Spice', 'best'],\n",
       " ['review', 'Sea', 'Salt', 'w', 'Kelp', 'soap'],\n",
       " ['used', 'Bellmira', 'Herbaflor', 'Herbal', 'Baths', 'years'],\n",
       " ['Thanks', 'Combe', 'Inc', 'END'],\n",
       " ['Lectric', 'Shave', 'END'],\n",
       " ['added', 'Lectric', 'Shave', 'green'],\n",
       " ['turned', 'Amazon', 'END'],\n",
       " ['used', 'Lectric', 'Shave', 'decades'],\n",
       " ['looking', 'Lectric', 'Shave', 'END'],\n",
       " ['used', 'Lectric', 'Shave', '1973'],\n",
       " ['1973', 'Mark', '4', 'Remington', 'few'],\n",
       " ['store', \"Norelco's\", 'one'],\n",
       " ['horrible', 'Sunbeam', 'now'],\n",
       " ['favorite', 'Braun', '320s'],\n",
       " ['Braun', '320s', '4', '#'],\n",
       " ['ms-3/4700', 'Remington', 'micro'],\n",
       " ['razor', 'Rem', 'ever'],\n",
       " ['great', 'Military', 'base'],\n",
       " ['Axe',\n",
       "  'Detailer',\n",
       "  'Shower',\n",
       "  'Tool',\n",
       "  'basically',\n",
       "  'Ax',\n",
       "  'Shower',\n",
       "  'Tool',\n",
       "  'get'],\n",
       " ['Clean', 'Clear', 'Cleanser', 'see'],\n",
       " ['START', 'Clean', 'Clear', 'sure'],\n",
       " ['use', 'Crest', 'children'],\n",
       " ['children', 'Colgate', 'better'],\n",
       " ['Colgate', 'Kids', 'Maximum', 'Cavity', 'Protection', 'END'],\n",
       " ['12', 'Cellulose', 'Gum', 'Sodium'],\n",
       " ['Gum', 'Sodium', 'Lauryl', 'Sulfate', 'Flavor'],\n",
       " ['Sulfate', 'Flavor', 'Sodium'],\n",
       " ['Mica', 'Titanium', 'Dioxide', 'D&C'],\n",
       " ['10', 'FD&C', 'Blue'],\n",
       " ['flavor', 'Colgate', 'Maximum', 'Cavity', 'Protection', 'toothpaste'],\n",
       " ['use', 'Colgate', 'Kids', 'toothpaste'],\n",
       " ['START', 'Colgate', 'favorite'],\n",
       " ['use', 'Optic', 'White', 'END'],\n",
       " ['review', 'Maximum', 'Cavity', 'Protection', 'growing'],\n",
       " ['toothpaste', 'Colgate', 'Optic', 'White', 'mouthwash'],\n",
       " ['using', 'Colgate', 'pumps'],\n",
       " ['pumps', 'Colgate', 'Maximum', 'Cavity', 'Protection', 'mild'],\n",
       " ['run', 'Glide', 'have'],\n",
       " ['using', 'Glide', 'floss'],\n",
       " ['original', 'Glide', 'GoreTex'],\n",
       " ['START', \"Sam's\", 'Club', 'member'],\n",
       " ['original', 'Glide', 'END'],\n",
       " ['try', 'Glide', 'Comfort', 'Unflavored', 'next'],\n",
       " ['example', 'Amazon', 'throws'],\n",
       " ['love', 'Pre', 'de', 'Provence', 'soaps'],\n",
       " ['one', 'Pre', 'de', 'Provence', 'product'],\n",
       " ['have', 'Pre', 'de', 'Provence', 'spray'],\n",
       " ['Pre',\n",
       "  'De',\n",
       "  'Provence',\n",
       "  'Maison',\n",
       "  'French',\n",
       "  'Lavender',\n",
       "  'Blossom',\n",
       "  'Linen',\n",
       "  'END'],\n",
       " ['Additionally', 'Maison', 'has'],\n",
       " ['MIT', 'Methylchloroisothiazolinone', 'CMIT'],\n",
       " ['CMIT', 'FD&C', 'Blue', '1', 'Red'],\n",
       " ['1', 'Red', '33', 'rather'],\n",
       " ['adore',\n",
       "  'Pre',\n",
       "  'De',\n",
       "  'Provence',\n",
       "  'product',\n",
       "  'Lavender',\n",
       "  'Blossom',\n",
       "  'Water',\n",
       "  'exception'],\n",
       " ['Bath', 'Salts', 'END'],\n",
       " ['START', 'Pre', 'De', 'Provence', 'Maison', 'French', 'Lavender', 'bath'],\n",
       " ['START', 'Selenium', 'appeared'],\n",
       " ['get', 'Amazon', 'END'],\n",
       " ['have', 'Airbnb', 'END'],\n",
       " ['Dave', 'Buster'],\n",
       " ['START', 'Uber', 'pleased', 'D&G', 'Lifht', 'Blue', 'one'],\n",
       " ['pleased', 'Uhuru', 'Naturals', 'Hair'],\n",
       " ['START', 'Google', 'uses'],\n",
       " ['well', 'Northern', 'Fir'],\n",
       " ['START', 'Eau', 'de', 'Hadrien', 'favorite'],\n",
       " ['Tangle', 'Taming', 'Conditioner', 'good'],\n",
       " ['tried', 'Kerastase', 'PHYTO'],\n",
       " ['Kerastase', 'PHYTO', 'Davines'],\n",
       " ['wish', 'Redken', 'discontinued'],\n",
       " ['Federal',\n",
       "  'Trade',\n",
       "  'Commission',\n",
       "  'Guidelines',\n",
       "  'Testimony',\n",
       "  'Advertising',\n",
       "  'END'],\n",
       " ['ago', 'Sally', 'Beauty', 'Supply', 'END'],\n",
       " ['PRELL', 'Conditioner', 'far'],\n",
       " ['company', 'Amazon', 'buy'],\n",
       " ['discontinued', 'Bain', 'De', 'Terre', 'END'],\n",
       " ['used', 'Mousse', 'products'],\n",
       " ['use', 'Pureology', 'END'],\n",
       " ['product', 'Pantene', 'made'],\n",
       " ['START', 'Perlier', 'makes'],\n",
       " ['bought', 'Big', 'Lots', 'only'],\n",
       " ...]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_word_ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6472e8d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "fbc8cb68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_metrics(our_ngram, all_text):\n",
    "    freq_of_bigram = count_freq_ngram(our_ngram, all_text)\n",
    "    left_context, center, right_context = count_freq(our_ngram, all_text)\n",
    "    num_bigrams = len(all_text) - 2\n",
    "    #было бы сложно иным образом оценить количество биграмм -- с одной стороны, у нас есть многоcловные успоминания,\n",
    "    #с другой -- там также могут быть фамилии и т.д. Это примерное число именно возможных биграмм, и оно не сильно \n",
    "    #отдаляется от настоящего числа биграмм на нашем довольно большом корпусе, кроме того, оно для всех одинаково\n",
    "    if freq_of_bigram != 0 and left_context != 0 and right_context != 0 and center != 0:\n",
    "        t_score_left = (freq_of_bigram - (left_context * center)/num_bigrams)/(freq_of_bigram**(1/2))\n",
    "        t_score_right = (freq_of_bigram - (right_context * center)/num_bigrams)/(freq_of_bigram**(1/2))\n",
    "        dice_left = (2*freq_of_bigram)/(left_context + center)\n",
    "        dice_right = (2*freq_of_bigram)/(right_context + center)\n",
    "        pmi_left = math.log2(freq_of_bigram / (left_context * center))\n",
    "        pmi_right = math.log2(freq_of_bigram / (right_context * center))\n",
    "        \n",
    "    else:\n",
    "        t_score_left, t_score_right, dice_left, dice_right, pmi_left, pmi_right = [0, 0, 0, 0, 0, 0]\n",
    "    \n",
    "    return t_score_left, t_score_right, dice_left, dice_right, pmi_left, pmi_right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "2dca8632",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1415/1415 [00:30<00:00, 46.97it/s]\n"
     ]
    }
   ],
   "source": [
    "t_scores_left, t_scores_right, dices_left, dices_right, pmis_left, pmis_right = [], [], [], [], [], []\n",
    "\n",
    "for ngram in tqdm(all_word_ngrams):\n",
    "    ngram = [word.lower() for word in ngram]\n",
    "    if len(ngram) > 2:\n",
    "        t_score_left, t_score_right, dice_left, dice_right, pmi_left, pmi_right = count_metrics(ngram, all_text)\n",
    "        t_scores_left.append(t_score_left)\n",
    "        t_scores_right.append(t_score_right)\n",
    "        dices_left.append(dice_left)\n",
    "        dices_right.append(dice_right)\n",
    "        pmis_left.append(pmi_left)\n",
    "        pmis_right.append(pmi_right)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "40e25bfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['prolong', 'Spongelle', 'useful']\n",
      "['used', 'Bellmira', 'Herbaflor', 'Herbal', 'Baths', 'years']\n",
      "['see', 'P&G', 'bring']\n",
      "['body', 'Spongelle', 'END']\n",
      "['get', 'Amazon', 'END']\n",
      "['START', 'Crest', 'Pro-Health', 'Life', 'helped']\n",
      "['Ethylhexylglycerin', 'Tocopherol', 'Limonene']\n",
      "['body', 'Spongelle', 'END']\n",
      "['gel', 'Solstis', 'really']\n",
      "['get', 'Amazon', 'END']\n",
      "['used', 'Bellmira', 'Herbaflor', 'Herbal', 'Baths', 'years']\n",
      "['Thanks', 'Amazon', 'no']\n",
      "['START', 'Eau', 'de', 'Hadrien', 'favorite']\n",
      "['get', 'Amazon', 'END']\n",
      "['START', 'Google', 'uses']\n",
      "['used', 'WaterPik', 'great']\n",
      "['tried', 'Kerastase', 'PHYTO']\n",
      "['shampoo', 'Detangler', 'Conditioner']\n",
      "['product', 'Sally', 'Beauty', 'Supply', 'END']\n",
      "['Shower', 'Gel', 'hoping']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "a = np.argsort(t_scores_left)[::-1][:20]\n",
    "for i in a:\n",
    "    print(all_word_ngrams[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "2238208a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['prolong', 'Spongelle', 'useful']\n",
      "['get', 'Amazon', 'END']\n",
      "['see', 'P&G', 'bring']\n",
      "['used', 'Bellmira', 'Herbaflor', 'Herbal', 'Baths', 'years']\n",
      "['body', 'Spongelle', 'END']\n",
      "['START', 'Eau', 'de', 'Hadrien', 'favorite']\n",
      "['used', 'WaterPik', 'great']\n",
      "['Thanks', 'Amazon', 'no']\n",
      "['tried', 'Kerastase', 'PHYTO']\n",
      "['get', 'Amazon', 'END']\n",
      "['START', 'Google', 'uses']\n",
      "['used', 'Bellmira', 'Herbaflor', 'Herbal', 'Baths', 'years']\n",
      "['body', 'Spongelle', 'END']\n",
      "['Ethylhexylglycerin', 'Tocopherol', 'Limonene']\n",
      "['get', 'Amazon', 'END']\n",
      "['gel', 'Solstis', 'really']\n",
      "['START', 'Crest', 'Pro-Health', 'Life', 'helped']\n",
      "['START', 'Crest', 'Pro-Health', 'Life', 'helped']\n",
      "['whey', 'Old', 'Spice', 'discontinued']\n",
      "['Tangle', 'Taming', 'Conditioner', 'good']\n"
     ]
    }
   ],
   "source": [
    "a = np.argsort(t_scores_right)[::-1][:20]\n",
    "for i in a:\n",
    "    print(all_word_ngrams[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "3fd45887",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['always', 'Bare', 'Minerals', 'never']\n",
      "['shipped', 'FPO', 'address']\n",
      "['Oil', 'Glycol', 'Distearate', 'Fragrance']\n",
      "['Distearate', 'Fragrance', 'Organic']\n",
      "['Chamomile', 'Althea', 'Sage']\n",
      "['Axe', 'Detailer', 'Shower', 'Tool', 'basically', 'Ax', 'Shower', 'Tool', 'get']\n",
      "['consider', 'Macallan', 'single']\n",
      "['turned', 'Amazon', 'END']\n",
      "['looking', 'Lectric', 'Shave', 'END']\n",
      "['made', 'Gillette', 'St.']\n",
      "['CMIT', 'FD&C', 'Blue', '1', 'Red']\n",
      "['Chamomile', 'Althea', 'Sage']\n",
      "['tried', 'Essie', 'polish']\n",
      "['Glycerin', 'Potassium', 'Hydroxide', 'Organic']\n",
      "['Hydroxide', 'Organic', 'Coconut', 'Oil', 'Glycol']\n",
      "['g', 'Traditional', 'Fragrance', 'Sandalwood', '/']\n",
      "['Bvlgari', 'Conditioner', 'favorite']\n",
      "['turned', 'Amazon', 'END']\n",
      "['Distearate', 'Fragrance', 'Organic']\n",
      "['Get', 'Grip', 'Trio', 'Manicure', 'Pedicure']\n"
     ]
    }
   ],
   "source": [
    "a = np.argsort(dices_left)[::-1][:20]\n",
    "for i in a:\n",
    "    print(all_word_ngrams[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "804154dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Alcohol', 'Pectin', 'Papaya']\n",
      "['single', 'Escada', 'fragrance']\n",
      "['Glycerin', 'Potassium', 'Hydroxide', 'Organic']\n",
      "['Oil', 'Cetearyl', 'Alcohol', 'Pectin']\n",
      "['points', 'QC', 'issue']\n",
      "['Oil', 'Glycol', 'Distearate', 'Fragrance']\n",
      "['use', 'Estee', 'Lauder', 'foundation']\n",
      "['Hydroxide', 'Organic', 'Coconut', 'Oil', 'Glycol']\n",
      "['Urban', 'Spa', 'Microfiber', 'Bath', 'Pillow', 'makes']\n",
      "['Water', 'Stearic', 'Acid', 'Myristic']\n",
      "['consider', 'Macallan', 'single']\n",
      "['tried', 'Essie', 'polish']\n",
      "['original', 'Gillette', 'equipment']\n",
      "['Hydroxide', 'Organic', 'Coconut', 'Oil', 'Glycol']\n",
      "['START', \"Sam's\", 'Club', 'member']\n",
      "['Lectric', 'Shave', 'END']\n",
      "['Get', 'Grip', 'Trio', 'Manicure', 'Pedicure']\n",
      "['get', 'Crest', 'pro']\n",
      "['Lectric', 'Shave', 'END']\n",
      "['Alcohol', '40', 'B', 'Bergamot']\n"
     ]
    }
   ],
   "source": [
    "a = np.argsort(dices_right)[::-1][:20]\n",
    "for i in a:\n",
    "    print(all_word_ngrams[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "65212c98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['other', 'Crest', 'Pro-Heath', 'products']\n",
      "['Braun', '320s', '4', '#']\n",
      "['Ethylhexyl', 'Octyl', 'Palmitate', 'Ethylhexylglycerin']\n",
      "['Ethylenediamine', 'Ethylhexyl', 'Octyl']\n",
      "['Phenoxyethanol', 'Tetrahydroxypropyl', 'Ethylenediamine', 'Ethylhexyl']\n",
      "['Fragrance', 'Organic', 'Jojoba', 'Seed', 'Oil', 'Organic', 'Pomegranate', 'Oil', 'Organic', 'Green', 'Tea', 'Leaf', 'Extract', 'Organic', 'Shea', 'Butter', 'Fruit', 'Phenoxyethanol']\n",
      "['brush', 'Urban', 'Spa', 'END']\n",
      "['adherence', 'FTC', 'regulations']\n",
      "['ms-3/4700', 'Remington', 'micro']\n",
      "['store', \"Norelco's\", 'one']\n",
      "['have', 'Airbnb', 'END']\n",
      "['review', 'Sea', 'Salt', 'w', 'Kelp', 'soap']\n",
      "['shower', 'Gel', 'END']\n",
      "['However', 'Tea', 'Tree', 'often']\n",
      "['day', 'SPA', 'LOVE']\n",
      "['buy', 'Kerastase', 'Fusio-', 'Dose', 'Systeme', 'Professionnel', 'END']\n",
      "['fan', 'Molton', 'Brown', 'products']\n",
      "['day', 'SPA', 'LOVE']\n",
      "['Palmitate', 'Ethylhexylglycerin', 'Tocopherol']\n",
      "['see', 'P&G', 'bring']\n"
     ]
    }
   ],
   "source": [
    "a = np.argsort(pmis_left)[::-1][:20]\n",
    "for i in a:\n",
    "    print(all_word_ngrams[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "c6dc4231",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['other', 'Crest', 'Pro-Heath', 'products']\n",
      "['again', 'Amazon', 'thanks']\n",
      "['Palmitate', 'Ethylhexylglycerin', 'Tocopherol']\n",
      "['Ethylhexyl', 'Octyl', 'Palmitate', 'Ethylhexylglycerin']\n",
      "['Ethylenediamine', 'Ethylhexyl', 'Octyl']\n",
      "['Phenoxyethanol', 'Tetrahydroxypropyl', 'Ethylenediamine', 'Ethylhexyl']\n",
      "['Fragrance', 'Organic', 'Jojoba', 'Seed', 'Oil', 'Organic', 'Pomegranate', 'Oil', 'Organic', 'Green', 'Tea', 'Leaf', 'Extract', 'Organic', 'Shea', 'Butter', 'Fruit', 'Phenoxyethanol']\n",
      "['brush', 'Urban', 'Spa', 'END']\n",
      "['adherence', 'FTC', 'regulations']\n",
      "['ms-3/4700', 'Remington', 'micro']\n",
      "['Braun', '320s', '4', '#']\n",
      "['store', \"Norelco's\", 'one']\n",
      "['review', 'Sea', 'Salt', 'w', 'Kelp', 'soap']\n",
      "['shower', 'Gel', 'END']\n",
      "['However', 'Tea', 'Tree', 'often']\n",
      "['day', 'SPA', 'LOVE']\n",
      "['buy', 'Kerastase', 'Fusio-', 'Dose', 'Systeme', 'Professionnel', 'END']\n",
      "['fan', 'Molton', 'Brown', 'products']\n",
      "['day', 'SPA', 'LOVE']\n",
      "['have', 'Airbnb', 'END']\n"
     ]
    }
   ],
   "source": [
    "a = np.argsort(pmis_right)[::-1][:20]\n",
    "for i in a:\n",
    "    print(all_word_ngrams[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f7d1ae",
   "metadata": {},
   "source": [
    "Метрика DICE с левым контекстом больше всех похожа на правду -- там больше всего названий продуктов. Без пунктуации и служебных частей речи всё выглядит гораздо более осмысленно. Со служебными частями речи левый контекст работал плохо, потому что там в английском предлоги, союзы и артикли. С включенной очисткой он стал более информативным, а в правый контекст попало больше составов, чем названий."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c75abdd",
   "metadata": {},
   "source": [
    "## Выводим тип продуктов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "10304b39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***************\n",
      "shampoo\n",
      "***************\n",
      "shampoo Pantene does\n",
      "shampoo Detangler Conditioner\n",
      "like H20 shampoo\n",
      "START Volume Conditioner shampoo\n",
      "classic Pantene Shampoo END\n",
      "original Matrix Amplifying / Volumizing System Shampoo number\n",
      "liked Pantene shampoo\n",
      "***************\n",
      "conditioner\n",
      "***************\n",
      "conditioner Pantene END\n",
      "PRELL Conditioner far\n",
      "use Matrix conditioner\n",
      "shampoo Detangler Conditioner\n",
      "START Volume Conditioner shampoo\n",
      "START Matrix Biolage Fortifying conditioner\n",
      "Tangle Taming Conditioner good\n",
      "START Fekkai Foam Conditioner unusual\n",
      "Bvlgari Conditioner favorite\n",
      "Blown Aerosol Foam Conditioner conditioner\n",
      "***************\n",
      "soap\n",
      "***************\n",
      "soap Kirkland body\n",
      "START Pre de Provence Shea Butter Enriched Artisanal French Soap Bar good\n",
      "supply Pre de Provence Soap Milk\n",
      "Soap Milk fantastic\n",
      "Artisanal French Soap Bar has\n",
      "bar Mint Leaf Pre de Provence Shea Butter Soap delightful\n",
      "fourth Pre de Provence soap\n",
      "bar Pre de Provence Butter Soap Violette\n",
      "loved Pre de Provence soaps\n",
      "START Pre de Provence soaps\n",
      "bar Plantlife Sandalwood soap Pre de Provence END\n",
      "sudzing Girley soap\n",
      "other Pre de Provence soaps\n",
      "thing Pre de Provence soaps\n",
      "scented Shea Butter soap\n",
      "get Dove soaps\n",
      "soaps Shea butter\n",
      "take Softsoap grain\n",
      "Soaps Company makes\n",
      "used Pre de Provence soaps\n",
      "kinds Dial Soap all\n",
      "buy Pre De Provence soaps\n",
      "soap Plantlife patchouli\n",
      "Pre-de-Provence-Shea-Butter-Enriched-Artisanal-French-Soap-Bar-200-g-Traditional-Fragrance-Sandalwood/dp/B001UFZOMS/ref=cm_cr_arp_d_rvw_txt?ie=UTF8 Pre de Provence Shea Butter Enriched Artisanal French Soap Bar 200\n",
      "love Pre de Provence soaps\n",
      "basic Ivory soap\n",
      "first Pre de Provence soaps\n",
      "safer Pre de Provence soap\n",
      "using Pre de Provence soaps\n",
      "Soap Violette fragrance\n",
      "review Sea Salt w Kelp soap\n",
      "Most Pre de Provence soaps\n",
      "Pre de Provence Soaps used Pre de Provence Verbena END\n",
      "START Pre de Provence Soap Milk\n",
      "like Pre de Provence soaps\n",
      "***************\n",
      "butter\n",
      "***************\n",
      "START Pre de Provence Shea Butter Enriched Artisanal French Soap Bar good\n",
      "bar Mint Leaf Pre de Provence Shea Butter Soap delightful\n",
      "bar Pre de Provence Butter Soap Violette\n",
      "START Pre De Provence Shea butter\n",
      "Oil Shea Butter Certified\n",
      "say Shea Butter scent\n",
      "scented Shea Butter soap\n",
      "soaps Shea butter\n",
      "lotions Shea Butter while\n",
      "START Pre de Provence Shea Butter Enriched\n",
      "avoid Sandalwood Shea Butter version\n",
      "Fragrance Organic Jojoba Seed Oil Organic Pomegranate Oil Organic Green Tea Leaf Extract Organic Shea Butter Fruit Phenoxyethanol\n",
      "also Shea Butter enriched\n",
      "Pre-de-Provence-Shea-Butter-Enriched-Artisanal-French-Soap-Bar-200-g-Traditional-Fragrance-Sandalwood/dp/B001UFZOMS/ref=cm_cr_arp_d_rvw_txt?ie=UTF8 Pre de Provence Shea Butter Enriched Artisanal French Soap Bar 200\n",
      "has Shea butter\n",
      "***************\n",
      "lotion\n",
      "***************\n",
      "original Creme Perfume tube Body Lotion END\n",
      "using Pre de Provence lotions\n",
      "lotions Shea Butter while\n",
      "love Clarins Eau Ensoleillante Moisturizing Body Lotion END\n",
      "lotion Ross store\n",
      "bottle J'Adore lotion\n"
     ]
    }
   ],
   "source": [
    "for word in ['shampoo', 'conditioner', 'soap', 'butter', 'lotion']:\n",
    "    print('***************')\n",
    "    print(word)\n",
    "    print('***************')\n",
    "    for ngram in set(' '.join(i) for i in all_word_ngrams):\n",
    "        if word in ngram.lower():\n",
    "            print(ngram)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f3d6018",
   "metadata": {},
   "source": [
    "Вот так гораздо лучше! Без служебных слов всё становится более осмысленным и информативным"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "422f10b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e41ae0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
